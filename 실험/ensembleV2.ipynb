{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ“Š ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ì•™ìƒë¸” ëª¨ë¸\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ê±´ë¬¼ë³„ ëª¨ë¸**ê³¼ **ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸**ì„ ê°ê° êµ¬í˜„í•˜ê³  ì•™ìƒë¸”í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ”§ ì£¼ìš” ì „ëµ\n",
        "1. **ì „ì²˜ë¦¬ ë°©ì‹ ì ìš©**: CDH, THI, WCT, ì£¼ê¸°ì„± í”¼ì²˜ ë“±\n",
        "2. **ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸**: 100ê°œ ê±´ë¬¼ ê°ê°ì— ëŒ€í•œ ì „ìš© ëª¨ë¸\n",
        "3. **ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸**: 10ê°œ ê±´ë¬¼ ìœ í˜•ë³„ ëª¨ë¸\n",
        "4. **ê°€ì¤‘ ì•™ìƒë¸”**: ê°œë³„ ê±´ë¬¼ ëª¨ë¸ 70% + ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ 30%\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost ë²„ì „: 1.6.1\n",
            "ğŸš€ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ì•™ìƒë¸” ëª¨ë¸ ì‹œì‘!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import sklearn\n",
        "import xgboost\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "import random as rn\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# ì‹œë“œ ì„¤ì •\n",
        "RANDOM_SEED = 2025\n",
        "np.random.seed(RANDOM_SEED)\n",
        "rn.seed(RANDOM_SEED)\n",
        "\n",
        "# XGBoost ë²„ì „ í™•ì¸\n",
        "print(f\"XGBoost ë²„ì „: {xgboost.__version__}\")\n",
        "\n",
        "print(\"ğŸš€ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ì•™ìƒë¸” ëª¨ë¸ ì‹œì‘!\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. í‰ê°€ í•¨ìˆ˜ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "def smape(gt, preds):\n",
        "    \"\"\"SMAPE (Symmetric Mean Absolute Percentage Error) ê³„ì‚°\"\"\"\n",
        "    gt = np.array(gt)\n",
        "    preds = np.array(preds)\n",
        "    v = 2 * abs(preds - gt) / (abs(preds) + abs(gt))\n",
        "    score = np.mean(v) * 100\n",
        "    return score\n",
        "    \n",
        "def weighted_mse(alpha=1):\n",
        "    \"\"\"ê°€ì¤‘ MSE ì†ì‹¤ í•¨ìˆ˜ (Under-predictionì— ë” í° í˜ë„í‹°)\"\"\"\n",
        "    def weighted_mse_fixed(label, pred):\n",
        "        residual = (label - pred).astype(\"float\")\n",
        "        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n",
        "        hess = np.where(residual > 0, 2 * alpha, 2.0)\n",
        "        return grad, hess\n",
        "    return weighted_mse_fixed\n",
        "\n",
        "def custom_smape(preds, dtrain):\n",
        "    \"\"\"XGBoostìš© SMAPE í‰ê°€ í•¨ìˆ˜\"\"\"\n",
        "    labels = dtrain.get_label()\n",
        "    return 'custom_smape', np.mean(2 * abs(preds - labels) / (abs(preds) + abs(labels))) * 100\n",
        "\n",
        "print(\"âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\n",
            "âœ… Train ë°ì´í„°: (204000, 10)\n",
            "âœ… Test ë°ì´í„°: (16800, 7)\n",
            "âœ… Building info: (100, 7)\n",
            "âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "building_info = pd.read_csv('data/building_info.csv')\n",
        "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
        "\n",
        "print(f\"âœ… Train ë°ì´í„°: {train.shape}\")\n",
        "print(f\"âœ… Test ë°ì´í„°: {test.shape}\")\n",
        "print(f\"âœ… Building info: {building_info.shape}\")\n",
        "\n",
        "# ì»¬ëŸ¼ëª… ì˜ì–´ë¡œ ë³€ê²½ (ì‘ë…„ ìˆ˜ìƒì ë°©ì‹)\n",
        "train = train.rename(columns={\n",
        "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
        "    'ì¼ì‹œ': 'date_time',\n",
        "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
        "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
        "    'í’ì†(m/s)': 'windspeed',\n",
        "    'ìŠµë„(%)': 'humidity',\n",
        "    'ì¼ì¡°(hr)': 'sunshine',\n",
        "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation',\n",
        "    'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'power_consumption'\n",
        "})\n",
        "train.drop('num_date_time', axis=1, inplace=True)\n",
        "\n",
        "test = test.rename(columns={\n",
        "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
        "    'ì¼ì‹œ': 'date_time',\n",
        "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
        "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
        "    'í’ì†(m/s)': 'windspeed',\n",
        "    'ìŠµë„(%)': 'humidity',\n",
        "    'ì¼ì¡°(hr)': 'sunshine',\n",
        "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation'\n",
        "})\n",
        "test.drop('num_date_time', axis=1, inplace=True)\n",
        "\n",
        "building_info = building_info.rename(columns={\n",
        "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
        "    'ê±´ë¬¼ìœ í˜•': 'building_type',\n",
        "    'ì—°ë©´ì (m2)': 'total_area',\n",
        "    'ëƒ‰ë°©ë©´ì (m2)': 'cooling_area',\n",
        "    'íƒœì–‘ê´‘ìš©ëŸ‰(kW)': 'solar_power_capacity',\n",
        "    'ESSì €ì¥ìš©ëŸ‰(kWh)': 'ess_capacity',\n",
        "    'PCSìš©ëŸ‰(kW)': 'pcs_capacity'\n",
        "})\n",
        "\n",
        "# ê±´ë¬¼ ìœ í˜• ì˜ì–´ë¡œ ë²ˆì—­\n",
        "translation_dict = {\n",
        "    'ê±´ë¬¼ê¸°íƒ€': 'Other Buildings',\n",
        "    'ê³µê³µ': 'Public',\n",
        "    'í•™êµ': 'University',\n",
        "    'ë°±í™”ì ': 'Department Store',\n",
        "    'ë³‘ì›': 'Hospital',\n",
        "    'ìƒìš©': 'Commercial',\n",
        "    'ì•„íŒŒíŠ¸': 'Apartment',\n",
        "    'ì—°êµ¬ì†Œ': 'Research Institute',\n",
        "    'IDC(ì „í™”êµ­)': 'IDC',\n",
        "    'í˜¸í…”': 'Hotel'\n",
        "}\n",
        "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
        "\n",
        "# íƒœì–‘ê´‘/ESS ì„¤ë¹„ ìœ ë¬´ í”¼ì²˜ ìƒì„±\n",
        "building_info['solar_power_utility'] = np.where(building_info.solar_power_capacity != '-', 1, 0)\n",
        "building_info['ess_utility'] = np.where(building_info.ess_capacity != '-', 1, 0)\n",
        "\n",
        "# ê±´ë¬¼ ì •ë³´ ë³‘í•©\n",
        "train = pd.merge(train, building_info, on='building_number', how='left')\n",
        "test = pd.merge(test, building_info, on='building_number', how='left')\n",
        "\n",
        "print(\"âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Feature Engineering (ì‘ë…„ ìˆ˜ìƒì ë°©ì‹)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Feature Engineering ì‹œì‘...\n",
            "âœ… ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë‚ ì§œ/ì‹œê°„ ë³€í™˜ ë° ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„±\n",
        "print(\"ğŸ”§ Feature Engineering ì‹œì‘...\")\n",
        "\n",
        "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
        "test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
        "\n",
        "# ê¸°ë³¸ ì‹œê°„ í”¼ì²˜\n",
        "for df in [train, test]:\n",
        "    df['hour'] = df['date_time'].dt.hour\n",
        "    df['day'] = df['date_time'].dt.day\n",
        "    df['month'] = df['date_time'].dt.month\n",
        "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
        "\n",
        "print(\"âœ… ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„±\n",
        "def calculate_day_values(dataframe, target_column, output_column, aggregation_func):\n",
        "    \"\"\"ì¼ë³„ í†µê³„ê°’ ê³„ì‚° í•¨ìˆ˜\"\"\"\n",
        "    result_dict = {}\n",
        "    grouped_temp = dataframe.groupby(['building_number', 'month', 'day'])[target_column].agg(aggregation_func)\n",
        "    \n",
        "    for (building, month, day), value in grouped_temp.items():\n",
        "        result_dict.setdefault(building, {}).setdefault(month, {})[day] = value\n",
        "    \n",
        "    dataframe[output_column] = [\n",
        "        result_dict.get(row['building_number'], {}).get(row['month'], {}).get(row['day'], None)\n",
        "        for _, row in dataframe.iterrows()\n",
        "    ]\n",
        "\n",
        "# ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„±\n",
        "for df in [train, test]:\n",
        "    calculate_day_values(df, 'temperature', 'day_max_temperature', 'max')\n",
        "    calculate_day_values(df, 'temperature', 'day_mean_temperature', 'mean')\n",
        "    calculate_day_values(df, 'temperature', 'day_min_temperature', 'min')\n",
        "    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']\n",
        "\n",
        "print(\"âœ… ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì œê±°í•  ì´ìƒì¹˜ ê°œìˆ˜: 68\n",
            "ë‚¨ì€ í–‰ ê°œìˆ˜: 203932\n",
            "âœ… ì´ìƒì¹˜ ì œê±° ë° ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ì´ìƒì¹˜ ì œê±° ë° ì¶”ê°€ í”¼ì²˜ ìƒì„±\n",
        "outlier_idx = train.index[train['power_consumption'] == 0].tolist()\n",
        "print(f\"ì œê±°í•  ì´ìƒì¹˜ ê°œìˆ˜: {len(outlier_idx)}\")\n",
        "train.drop(index=outlier_idx, inplace=True)\n",
        "print(f\"ë‚¨ì€ í–‰ ê°œìˆ˜: {train.shape[0]}\")\n",
        "\n",
        "# ê³µíœ´ì¼ í”¼ì²˜ ìƒì„±\n",
        "holi_weekday = ['2024-06-06', '2024-08-15']\n",
        "train['holiday'] = np.where(\n",
        "    (train.day_of_week >= 5) | (train.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday)), 1, 0\n",
        ")\n",
        "test['holiday'] = np.where(\n",
        "    (test.day_of_week >= 5) | (test.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday)), 1, 0\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„± (Cyclical Features)\n",
        "for df in [train, test]:\n",
        "    # ì‹œê°„ ì£¼ê¸°ì„±\n",
        "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 23.0)\n",
        "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 23.0)\n",
        "    \n",
        "    # ë‚ ì§œ ì£¼ê¸°ì„±\n",
        "    df['sin_date'] = -np.sin(2 * np.pi * (df['month'] + df['day'] / 31) / 12)\n",
        "    df['cos_date'] = -np.cos(2 * np.pi * (df['month'] + df['day'] / 31) / 12)\n",
        "    \n",
        "    # ì›” ì£¼ê¸°ì„±\n",
        "    df['sin_month'] = -np.sin(2 * np.pi * df['month'] / 12.0)\n",
        "    df['cos_month'] = -np.cos(2 * np.pi * df['month'] / 12.0)\n",
        "    \n",
        "    # ìš”ì¼ ì£¼ê¸°ì„±\n",
        "    df['sin_dayofweek'] = -np.sin(2 * np.pi * (df['day_of_week'] + 1) / 7.0)\n",
        "    df['cos_dayofweek'] = -np.cos(2 * np.pi * (df['day_of_week'] + 1) / 7.0)\n",
        "\n",
        "print(\"âœ… ì´ìƒì¹˜ ì œê±° ë° ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_inferred_holidays(df, building_info):\n",
        "    # ê·œì¹™ì´ ì—†ëŠ” ë°±í™”ì  ê±´ë¬¼ ë²ˆí˜¸ ì¶”ì¶œ\n",
        "    department_store_buildings = [19,34,45,54,73,74,79,88,95]\n",
        "\n",
        "    # ë°±í™”ì  ê±´ë¬¼ì— ëŒ€í•´ ì¶”ì • íœ´ì¼ë§Œ ì ìš©\n",
        "    for building_num in department_store_buildings:\n",
        "        building_data = df[df['building_number'] == building_num].copy()\n",
        "\n",
        "        if len(building_data) > 0:\n",
        "            # ì¼ë³„ ì „ë ¥ì†Œë¹„ëŸ‰ ê³„ì‚°\n",
        "            building_data['date'] = building_data['date_time'].dt.date\n",
        "            daily_consumption = building_data.groupby('date')['power_consumption'].sum()\n",
        "\n",
        "            # ì „ì²´ í‰ê·  ê¸°ì¤€ ì„ê³„ì¹˜ (0.7ë°°)\n",
        "            threshold = daily_consumption.mean() * 0.7\n",
        "            inferred_holiday_dates = daily_consumption[daily_consumption < threshold].index\n",
        "\n",
        "            # ì¶”ì • íœ´ì¼ì„ í•´ë‹¹ ê±´ë¬¼ì˜ holiday ì»¬ëŸ¼ì— ì ìš©\n",
        "            for holiday_date in inferred_holiday_dates:\n",
        "                mask = (df['building_number'] == building_num) & (df['date_time'].dt.date == holiday_date)\n",
        "                df.loc[mask, 'holiday'] = 1\n",
        "\n",
        "    return df\n",
        "\n",
        "# íœ´ì¼ í”¼ì²˜ ìƒì„±\n",
        "train = create_inferred_holidays(train, building_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì¤‘... (ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨)\n",
            "   ê±´ë¬¼ 18: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 27: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 40: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 59: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 63: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 29: ë§¤ë‹¬ 10ì¼ íœ´ì¼ 0ê°œ, 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 32: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "âœ… test ë°ì´í„°ì— ë°±í™”ì  ì¶”ì • íœ´ì¼ ì ìš© ì™„ë£Œ\n",
            "âœ… test ë°ì´í„°ì— íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼(18ë²ˆ) ì¶”ê°€ ë° ê¸°ì¡´ ê·œì¹™ ìœ ì§€\n",
        "\n",
        "def apply_specific_building_holidays(df):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ê±´ë¬¼ë“¤ì˜ íœ´ì¼ ê·œì¹™ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜ (ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚°)\n",
        "    - 18ë²ˆ ê±´ë¬¼: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´\n",
        "    - 27, 40, 63ë²ˆ ê±´ë¬¼: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ë¬´\n",
        "    - 29ë²ˆ ê±´ë¬¼: ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ íœ´ë¬´\n",
        "    - 32ë²ˆ ê±´ë¬¼: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ë¬´\n",
        "    \"\"\"\n",
        "    print(\"ğŸ”§ íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì¤‘... (ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨)\")\n",
        "    department_store_buildings = building_info[building_info['building_type'] == 'Department Store']['building_number'].tolist()\n",
        "\n",
        "    # ë°±í™”ì  ê±´ë¬¼ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ holidayë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
        "    df.loc[df['building_number'].isin(department_store_buildings), 'holiday'] = 0\n",
        "\n",
        "    # ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ (18ë²ˆ)\n",
        "    every_sunday_buildings = [18]\n",
        "\n",
        "    # í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ë“¤ (27, 40, 59, 63)\n",
        "    odd_week_sunday_buildings = [27, 40, 59, 63]\n",
        "\n",
        "    # ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ (29)\n",
        "    special_holiday_building = [29]\n",
        "\n",
        "    # í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ (32)\n",
        "    odd_week_monday_buildings = [32]\n",
        "\n",
        "    # ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚° (ì¼ìš”ì¼ ì‹œì‘)\n",
        "    def get_calendar_week_of_month(date_series):\n",
        "        \"\"\"ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚° (ì›”ì˜ ì²«ë‚ ì´ í¬í•¨ëœ ì£¼ë¥¼ 1ì£¼ì°¨ë¡œ)\"\"\"\n",
        "        result = []\n",
        "        for date in date_series:\n",
        "            first_day = date.replace(day=1)\n",
        "            first_day_weekday = first_day.weekday()\n",
        "            days_to_week_start = (first_day_weekday + 1) % 7  # ì¼ìš”ì¼ ê¸°ì¤€\n",
        "            week_start_of_first = first_day - pd.Timedelta(days=days_to_week_start)\n",
        "            days_since_first_week_start = (date - week_start_of_first).days\n",
        "            week_num = (days_since_first_week_start // 7) + 1\n",
        "            result.append(week_num)\n",
        "        return result\n",
        "\n",
        "    # ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚°\n",
        "    df['calendar_week_of_month'] = get_calendar_week_of_month(df['date_time'])\n",
        "\n",
        "    # 0. ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (18ë²ˆ ê±´ë¬¼)\n",
        "    for building_num in every_sunday_buildings:\n",
        "        mask = (df['building_number'] == building_num) & (df['day_of_week'] == 6)\n",
        "        df.loc[mask, 'holiday'] = 1\n",
        "        count = mask.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ì¼ {count}ê°œ ì ìš©\")\n",
        "\n",
        "    # 1. í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (27, 40, 59, 63ë²ˆ ê±´ë¬¼)\n",
        "    for building_num in odd_week_sunday_buildings:\n",
        "        mask = (df['building_number'] == building_num) & \\\n",
        "               (df['day_of_week'] == 6) & \\\n",
        "               (df['calendar_week_of_month'] % 2 == 1)\n",
        "        df.loc[mask, 'holiday'] = 1\n",
        "        count = mask.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ {count}ê°œ ì ìš©\")\n",
        "\n",
        "    # 2. ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (29ë²ˆ ê±´ë¬¼)\n",
        "    for building_num in special_holiday_building:\n",
        "        mask_10th = (df['building_number'] == building_num) & (df['date_time'].dt.day == 10)\n",
        "        df.loc[mask_10th, 'holiday'] = 1\n",
        "        count_10th = mask_10th.sum()\n",
        "        mask_5th_sunday = (df['building_number'] == building_num) & \\\n",
        "                          (df['day_of_week'] == 6) & \\\n",
        "                          (df['calendar_week_of_month'] == 5)\n",
        "        df.loc[mask_5th_sunday, 'holiday'] = 1\n",
        "        count_5th_sunday = mask_5th_sunday.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: ë§¤ë‹¬ 10ì¼ íœ´ì¼ {count_10th}ê°œ, 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ {count_5th_sunday}ê°œ ì ìš©\")\n",
        "\n",
        "    # 3. í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ë¬´ (32ë²ˆ ê±´ë¬¼)\n",
        "    for building_num in odd_week_monday_buildings:\n",
        "        mask = (df['building_number'] == building_num) & \\\n",
        "               (df['day_of_week'] == 0) & \\\n",
        "               (df['calendar_week_of_month'] % 2 == 1)\n",
        "        df.loc[mask, 'holiday'] = 1\n",
        "        count = mask.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ì¼ {count}ê°œ ì ìš©\")\n",
        "\n",
        "    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°\n",
        "    df.drop(['calendar_week_of_month'], axis=1, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# test ë°ì´í„°ì— íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš©\n",
        "test = apply_specific_building_holidays(test)\n",
        "\n",
        "print(\"âœ… test ë°ì´í„°ì— ë°±í™”ì  ì¶”ì • íœ´ì¼ ì ìš© ì™„ë£Œ\")\n",
        "print(\"âœ… test ë°ì´í„°ì— íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” íŠ¹ì • ê±´ë¬¼ë“¤ì˜ íœ´ì¼ ì ìš© ê²°ê³¼ í™•ì¸:\n",
            "ê±´ë¬¼ 27ë²ˆ (í™€ìˆ˜ ì£¼ ì¼ìš”ì¼) íœ´ì¼: 1ì¼\n",
            "ë‚ ì§œ: [datetime.date(2024, 8, 25)]...\n",
            "ê±´ë¬¼ 29ë²ˆ (ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼) íœ´ì¼: 1ì¼\n",
            "ë‚ ì§œ: [datetime.date(2024, 8, 25)]...\n",
            "ê±´ë¬¼ 32ë²ˆ (í™€ìˆ˜ ì£¼ ì›”ìš”ì¼) íœ´ì¼: 1ì¼\n",
            "ë‚ ì§œ: [datetime.date(2024, 8, 26)]...\n"
          ]
        }
      ],
      "source": [
        "# íŠ¹ì • ê±´ë¬¼ë“¤ì˜ íœ´ì¼ ì ìš© ê²°ê³¼ í™•ì¸\n",
        "print(\"ğŸ” íŠ¹ì • ê±´ë¬¼ë“¤ì˜ íœ´ì¼ ì ìš© ê²°ê³¼ í™•ì¸:\")\n",
        "\n",
        "# 27ë²ˆ ê±´ë¬¼ (í™€ìˆ˜ ì£¼ ì¼ìš”ì¼) íœ´ì¼ í™•ì¸\n",
        "holiday_dates_27 = test.loc[(test['building_number'] == 27) & (test['holiday'] == 1), 'date_time'].dt.date\n",
        "unique_holiday_dates_27 = holiday_dates_27.drop_duplicates().tolist()\n",
        "print(f\"ê±´ë¬¼ 27ë²ˆ (í™€ìˆ˜ ì£¼ ì¼ìš”ì¼) íœ´ì¼: {len(unique_holiday_dates_27)}ì¼\")\n",
        "print(f\"ë‚ ì§œ: {unique_holiday_dates_27[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "\n",
        "# 29ë²ˆ ê±´ë¬¼ (ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼) íœ´ì¼ í™•ì¸  \n",
        "holiday_dates_29 = test.loc[(test['building_number'] == 29) & (test['holiday'] == 1), 'date_time'].dt.date\n",
        "unique_holiday_dates_29 = holiday_dates_29.drop_duplicates().tolist()\n",
        "print(f\"ê±´ë¬¼ 29ë²ˆ (ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼) íœ´ì¼: {len(unique_holiday_dates_29)}ì¼\")\n",
        "print(f\"ë‚ ì§œ: {unique_holiday_dates_29[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "\n",
        "# 32ë²ˆ ê±´ë¬¼ (í™€ìˆ˜ ì£¼ ì›”ìš”ì¼) íœ´ì¼ í™•ì¸\n",
        "holiday_dates_32 = test.loc[(test['building_number'] == 32) & (test['holiday'] == 1), 'date_time'].dt.date\n",
        "unique_holiday_dates_32 = holiday_dates_32.drop_duplicates().tolist()\n",
        "print(f\"ê±´ë¬¼ 32ë²ˆ (í™€ìˆ˜ ì£¼ ì›”ìš”ì¼) íœ´ì¼: {len(unique_holiday_dates_32)}ì¼\")\n",
        "print(f\"ë‚ ì§œ: {unique_holiday_dates_32[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ê±´ë¬¼ 59ë²ˆ (í™€ìˆ˜ ì£¼ ì›”ìš”ì¼) íœ´ì¼: 1ì¼\n",
            "ë‚ ì§œ: [datetime.date(2024, 8, 25)]...\n"
          ]
        }
      ],
      "source": [
        "n  = 59\n",
        "holiday_dates_n = test.loc[(test['building_number'] == n) & (test['holiday'] == 1), 'date_time'].dt.date\n",
        "unique_holiday_dates_n = holiday_dates_n.drop_duplicates().tolist()\n",
        "print(f\"ê±´ë¬¼ {n}ë²ˆ (í™€ìˆ˜ ì£¼ ì›”ìš”ì¼) íœ´ì¼: {len(unique_holiday_dates_n)}ì¼\")\n",
        "print(f\"ë‚ ì§œ: {unique_holiday_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ê¸°ìƒ ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ìƒ ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„±\n",
        "def CDH(xs):\n",
        "    \"\"\"Cooling Degree Hours ê³„ì‚°\"\"\"\n",
        "    cumsum = np.cumsum(xs - 26)\n",
        "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
        "\n",
        "def calculate_and_add_cdh(dataframe):\n",
        "    \"\"\"ê±´ë¬¼ë³„ CDH ê³„ì‚° ë° ì¶”ê°€\"\"\"\n",
        "    cdhs = []\n",
        "    for i in range(1, 101):\n",
        "        temp = dataframe[dataframe['building_number'] == i]['temperature'].values\n",
        "        cdh = CDH(temp)\n",
        "        cdhs.append(cdh)\n",
        "    return np.concatenate(cdhs)\n",
        "\n",
        "# CDH, THI, WCT í”¼ì²˜ ìƒì„±\n",
        "train['CDH'] = calculate_and_add_cdh(train)\n",
        "test['CDH'] = calculate_and_add_cdh(test)\n",
        "\n",
        "# THI (Temperature Humidity Index)\n",
        "train['THI'] = 9/5 * train['temperature'] - 0.55 * (1 - train['humidity']/100) * (9/5 * train['temperature'] - 26) + 32\n",
        "test['THI'] = 9/5 * test['temperature'] - 0.55 * (1 - test['humidity']/100) * (9/5 * test['temperature'] - 26) + 32\n",
        "\n",
        "# WCT (Wind Chill Temperature)\n",
        "train['WCT'] = 13.12 + 0.6125 * train['temperature'] - 11.37 * (train['windspeed']**0.16) + 0.3965 * (train['windspeed']**0.16) * train['temperature']\n",
        "test['WCT'] = 13.12 + 0.6125 * test['temperature'] - 11.37 * (test['windspeed']**0.16) + 0.3965 * (test['windspeed']**0.16) * test['temperature']\n",
        "\n",
        "print(\"âœ… ê¸°ìƒ ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì¤‘...\n",
            "âœ… ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n",
            "ìµœì¢… train ë°ì´í„° shape: (203932, 41)\n",
            "ìµœì¢… test ë°ì´í„° shape: (16800, 38)\n"
          ]
        }
      ],
      "source": [
        "# ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± (Target-like Features)\n",
        "print(\"ğŸ“Š ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ê±´ë¬¼ë³„ ì‹œê°„ëŒ€/ìš”ì¼ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
        "power_mean = pd.pivot_table(train, values='power_consumption', \n",
        "                           index=['building_number', 'hour', 'day_of_week'], \n",
        "                           aggfunc=np.mean).reset_index()\n",
        "power_mean.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_mean']\n",
        "\n",
        "power_std = pd.pivot_table(train, values='power_consumption', \n",
        "                          index=['building_number', 'hour', 'day_of_week'], \n",
        "                          aggfunc=np.std).reset_index()\n",
        "power_std.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_std']\n",
        "\n",
        "# ê±´ë¬¼ë³„ ì‹œê°„ëŒ€ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
        "power_hour_mean = pd.pivot_table(train, values='power_consumption', \n",
        "                                index=['building_number', 'hour'], \n",
        "                                aggfunc=np.mean).reset_index()\n",
        "power_hour_mean.columns = ['building_number', 'hour', 'hour_mean']\n",
        "\n",
        "power_hour_std = pd.pivot_table(train, values='power_consumption', \n",
        "                               index=['building_number', 'hour'], \n",
        "                               aggfunc=np.std).reset_index()\n",
        "power_hour_std.columns = ['building_number', 'hour', 'hour_std']\n",
        "\n",
        "# í†µê³„ í”¼ì²˜ ë³‘í•©\n",
        "train = train.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "train = train.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "train = train.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
        "train = train.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
        "\n",
        "test = test.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "test = test.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "test = test.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
        "test = test.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "print(\"âœ… ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"ìµœì¢… train ë°ì´í„° shape: {train.shape}\")\n",
        "print(f\"ìµœì¢… test ë°ì´í„° shape: {test.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. ëª¨ë¸ë§ìš© ë°ì´í„° ì¤€ë¹„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ëª¨ë¸ë§ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\n",
            "í”¼ì²˜ ìˆ˜: 29\n",
            "ê±´ë¬¼ ìœ í˜• ìˆ˜: 10\n",
            "ê±´ë¬¼ ìˆ˜: 100\n",
            "ê±´ë¬¼ ìœ í˜•: ['Hotel' 'Commercial' 'Hospital' 'University' 'Other Buildings'\n",
            " 'Apartment' 'Research Institute' 'Department Store' 'IDC' 'Public']\n",
            "ê±´ë¬¼ ë²ˆí˜¸ ë²”ìœ„: 1 ~ 100\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë¸ë§ìš© í”¼ì²˜ ì„ íƒ (ì‘ë…„ ìˆ˜ìƒì ë°©ì‹)\n",
        "drop_columns = [\n",
        "    'solar_power_capacity', 'ess_capacity', 'pcs_capacity',\n",
        "    'power_consumption', 'rainfall', 'sunshine', 'solar_radiation',\n",
        "    'hour', 'day', 'month', 'day_of_week', 'date_time'\n",
        "]\n",
        "\n",
        "X = train.drop(drop_columns, axis=1)\n",
        "Y = train[['building_type', 'power_consumption']]\n",
        "test_X = test.drop([col for col in drop_columns if col in test.columns], axis=1)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ë§ìš© ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
        "print(f\"í”¼ì²˜ ìˆ˜: {X.shape[1]}\")\n",
        "print(f\"ê±´ë¬¼ ìœ í˜• ìˆ˜: {len(X['building_type'].unique())}\")\n",
        "print(f\"ê±´ë¬¼ ìˆ˜: {len(X['building_number'].unique())}\")\n",
        "\n",
        "# ê±´ë¬¼ ìœ í˜• ë¦¬ìŠ¤íŠ¸\n",
        "type_list = X[\"building_type\"].unique()\n",
        "building_list = X[\"building_number\"].unique()\n",
        "\n",
        "print(f\"ê±´ë¬¼ ìœ í˜•: {type_list}\")\n",
        "print(f\"ê±´ë¬¼ ë²ˆí˜¸ ë²”ìœ„: {min(building_list)} ~ {max(building_list)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ (Model 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¢ Department Store ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "==================================================\n",
            "\n",
            "ğŸ” ê±´ë¬¼ ìœ í˜•: Department Store\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 32636ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 2688ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.6176\n",
            "\n",
            "ğŸ¯ Department Store ëª¨ë¸ ì „ì²´ SMAPE: 3.6176\n",
            "âœ… Department Store ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "## ë°±í™”ì ë§Œ train\n",
        "print(\"ğŸ¢ Department Store ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "KFOLD_SPLITS = 7\n",
        "kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# ë°±í™”ì  ì „ìš© ê²°ê³¼ ì €ì¥ìš© DataFrame\n",
        "dept_mask_test = test_X['building_number'].isin(building_info[building_info['building_type'] == 'Department Store']['building_number'])\n",
        "dept_mask_train = X['building_number'].isin(building_info[building_info['building_type'] == 'Department Store']['building_number'])\n",
        "\n",
        "type_model_predictions = pd.DataFrame(index=test_X[dept_mask_test].index, columns=[\"answer\"], dtype=float)\n",
        "type_model_oof = pd.DataFrame(index=X[dept_mask_train].index, columns=[\"pred\"], dtype=float)\n",
        "\n",
        "type_model_scores = {}\n",
        "\n",
        "btype = \"Department Store\"\n",
        "print(f\"\\nğŸ” ê±´ë¬¼ ìœ í˜•: {btype}\")\n",
        "\n",
        "# í•´ë‹¹ ìœ í˜• ë°ì´í„° í•„í„°ë§\n",
        "x = X[X['building_type'] == btype].copy()\n",
        "y = Y[Y['building_type'] == btype]['power_consumption'].copy()\n",
        "xt = test_X[test_X['building_type'] == btype].copy()\n",
        "\n",
        "print(f\"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(x)}ê°œ\")\n",
        "print(f\"   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(xt)}ê°œ\")\n",
        "\n",
        "# ê±´ë¬¼ ë²ˆí˜¸ ì›-í•« ì¸ì½”ë”©\n",
        "x = pd.get_dummies(x, columns=[\"building_number\"], drop_first=False)\n",
        "xt = pd.get_dummies(xt, columns=[\"building_number\"], drop_first=False)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì—†ëŠ” ì»¬ëŸ¼ ì²˜ë¦¬\n",
        "xt = xt.reindex(columns=x.columns, fill_value=0)\n",
        "\n",
        "# building_type ì»¬ëŸ¼ ì œê±°\n",
        "x = x.drop(columns=[\"building_type\"])\n",
        "xt = xt.drop(columns=[\"building_type\"])\n",
        "\n",
        "# K-Fold êµì°¨ ê²€ì¦\n",
        "preds_valid = pd.Series(index=y.index, dtype=float)\n",
        "preds_test = []\n",
        "fold_scores = []\n",
        "\n",
        "x_values = x.values\n",
        "y_values = y.values\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(kf.split(x_values), 1):\n",
        "    X_tr, X_va = x_values[tr_idx], x_values[va_idx]\n",
        "    y_tr, y_va = y_values[tr_idx], y_values[va_idx]\n",
        "    \n",
        "    # ë¡œê·¸ ë³€í™˜\n",
        "    y_tr_log = np.log(y_tr)\n",
        "    y_va_log = np.log(y_va)\n",
        "    \n",
        "    # XGBoost ëª¨ë¸ í›ˆë ¨\n",
        "    model = XGBRegressor(\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=5000,\n",
        "        max_depth=10,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.5,\n",
        "        min_child_weight=3,\n",
        "        random_state=RANDOM_SEED,\n",
        "        objective=weighted_mse(3),\n",
        "        early_stopping_rounds=100,\n",
        "    )\n",
        "    \n",
        "    model.fit(\n",
        "        X_tr, y_tr_log,\n",
        "        eval_set=[(X_va, y_va_log)],\n",
        "        eval_metric=custom_smape,\n",
        "        verbose=False,\n",
        "    )\n",
        "    \n",
        "    # ê²€ì¦ ì˜ˆì¸¡ (ë¡œê·¸ ì—­ë³€í™˜)\n",
        "    va_pred = np.exp(model.predict(X_va))\n",
        "    preds_valid.iloc[va_idx] = va_pred\n",
        "    \n",
        "    # ì„±ëŠ¥ ê³„ì‚°\n",
        "    fold_smape = smape(y_va, va_pred)\n",
        "    fold_scores.append(fold_smape)\n",
        "    \n",
        "    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
        "    preds_test.append(np.exp(model.predict(xt.values)))\n",
        "\n",
        "# ê²€ì¦ ì˜ˆì¸¡ ì €ì¥\n",
        "type_model_oof.loc[preds_valid.index, \"pred\"] = preds_valid\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ì•™ìƒë¸” í‰ê· ) ì €ì¥\n",
        "type_model_predictions.loc[xt.index, \"answer\"] = np.mean(preds_test, axis=0)\n",
        "\n",
        "# ì„±ëŠ¥ ì €ì¥\n",
        "avg_smape = np.mean(fold_scores)\n",
        "type_model_scores[btype] = avg_smape\n",
        "\n",
        "print(f\"   ğŸ† í‰ê·  SMAPE: {avg_smape:.4f}\")\n",
        "\n",
        "# ë°±í™”ì  ì„±ëŠ¥ ê³„ì‚° (NaN ê°’ ì œê±°)\n",
        "dept_y_true = Y[dept_mask_train][\"power_consumption\"].values\n",
        "dept_y_pred = type_model_oof[\"pred\"].values\n",
        "\n",
        "# NaN ê°’ ì œê±°\n",
        "valid_mask = ~(pd.isna(dept_y_true) | pd.isna(dept_y_pred))\n",
        "if np.sum(valid_mask) > 0:\n",
        "    total_type_smape = smape(dept_y_true[valid_mask], dept_y_pred[valid_mask])\n",
        "else:\n",
        "    total_type_smape = float('nan')\n",
        "\n",
        "print(f\"\\nğŸ¯ Department Store ëª¨ë¸ ì „ì²´ SMAPE: {total_type_smape:.4f}\")\n",
        "print(\"âœ… Department Store ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ (Model 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ  ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "==================================================\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 18 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.1663\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 19 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 4.6571\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 27 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.1070\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 29 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2037ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 1.9587\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 32 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.3886\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 34 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 1.9461\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 40 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 2.9426\n",
            "   â³ ì§„í–‰ë¥ : 250.0% | í‰ê·  SMAPE: 3.0238\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 45 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.3811\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 54 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 6.4161\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 59 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.8745\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 63 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 2.7206\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 73 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 2.8266\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 74 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.7127\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 79 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.1505\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 88 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2039ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 3.4799\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 95 í›ˆë ¨ ì¤‘...\n",
            "   ğŸ“Š í›ˆë ¨ ë°ì´í„°: 2040ê°œ\n",
            "   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 168ê°œ\n",
            "   ğŸ† í‰ê·  SMAPE: 4.6334\n",
            "\n",
            "ğŸ¯ ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ ì „ì²´ SMAPE: 3.4604\n",
            "âœ… ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "## ë°±í™”ì  ë§Œ train\n",
        "\n",
        "print(\"ğŸ  ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 'Department Store' ê±´ë¬¼ ë²ˆí˜¸ë§Œ ì¶”ì¶œ\n",
        "department_store_buildings = building_info[building_info['building_type'] == 'Department Store']['building_number'].tolist()\n",
        "\n",
        "# ë°±í™”ì  ì „ìš© ê²°ê³¼ ì €ì¥ìš© DataFrame (ê¸°ì¡´ì— ì •ì˜ëœ ë§ˆìŠ¤í¬ ì¬ì‚¬ìš©)\n",
        "individual_model_predictions = pd.DataFrame(index=test_X[dept_mask_test].index, columns=[\"answer\"], dtype=float)\n",
        "individual_model_oof = pd.DataFrame(index=X[dept_mask_train].index, columns=[\"pred\"], dtype=float)\n",
        "\n",
        "individual_model_scores = {}\n",
        "\n",
        "for building_num in sorted(department_store_buildings):\n",
        "    print(f\"\\nğŸ¢ ê±´ë¬¼ {building_num} í›ˆë ¨ ì¤‘...\")\n",
        "    \n",
        "    # í•´ë‹¹ ê±´ë¬¼ ë°ì´í„° í•„í„°ë§\n",
        "    x = X[X['building_number'] == building_num].copy()\n",
        "    y = Y[Y.index.isin(x.index)]['power_consumption'].copy()\n",
        "    xt = test_X[test_X['building_number'] == building_num].copy()\n",
        "    \n",
        "    print(f\"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(x)}ê°œ\")\n",
        "    print(f\"   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(xt)}ê°œ\")\n",
        "    \n",
        "    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (ê±´ë¬¼ë²ˆí˜¸, ê±´ë¬¼ìœ í˜•)\n",
        "    feature_cols = [col for col in x.columns if col not in ['building_number', 'building_type']]\n",
        "    x_features = x[feature_cols].copy()\n",
        "    xt_features = xt[feature_cols].copy()\n",
        "    \n",
        "    # K-Fold êµì°¨ ê²€ì¦\n",
        "    preds_valid = pd.Series(index=y.index, dtype=float)\n",
        "    preds_test = []\n",
        "    fold_scores = []\n",
        "    \n",
        "    x_values = x_features.values\n",
        "    y_values = y.values\n",
        "    \n",
        "    for fold, (tr_idx, va_idx) in enumerate(kf.split(x_values), 1):\n",
        "        X_tr, X_va = x_values[tr_idx], x_values[va_idx]\n",
        "        y_tr, y_va = y_values[tr_idx], y_values[va_idx]\n",
        "        \n",
        "        # ë¡œê·¸ ë³€í™˜\n",
        "        y_tr_log = np.log(y_tr)\n",
        "        y_va_log = np.log(y_va)\n",
        "        \n",
        "        # XGBoost ëª¨ë¸ í›ˆë ¨\n",
        "        model = XGBRegressor(\n",
        "            learning_rate=0.05,\n",
        "            n_estimators=5000,\n",
        "            max_depth=10,\n",
        "            subsample=0.7,\n",
        "            colsample_bytree=0.5,\n",
        "            min_child_weight=3,\n",
        "            random_state=RANDOM_SEED,\n",
        "            objective=weighted_mse(3),\n",
        "            early_stopping_rounds=100,\n",
        "        )\n",
        "        \n",
        "        model.fit(\n",
        "            X_tr, y_tr_log,\n",
        "            eval_set=[(X_va, y_va_log)],\n",
        "            eval_metric=custom_smape,\n",
        "            verbose=False,\n",
        "        )\n",
        "        \n",
        "        # ê²€ì¦ ì˜ˆì¸¡ (ë¡œê·¸ ì—­ë³€í™˜)\n",
        "        va_pred = np.exp(model.predict(X_va))\n",
        "        preds_valid.iloc[va_idx] = va_pred\n",
        "        \n",
        "        # ì„±ëŠ¥ ê³„ì‚°\n",
        "        fold_smape = smape(y_va, va_pred)\n",
        "        fold_scores.append(fold_smape)\n",
        "        \n",
        "        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
        "        preds_test.append(np.exp(model.predict(xt_features.values)))\n",
        "    \n",
        "    # ê²€ì¦ ì˜ˆì¸¡ ì €ì¥\n",
        "    individual_model_oof.loc[preds_valid.index, \"pred\"] = preds_valid\n",
        "    \n",
        "    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ì•™ìƒë¸” í‰ê· ) ì €ì¥\n",
        "    individual_model_predictions.loc[xt.index, \"answer\"] = np.mean(preds_test, axis=0)\n",
        "    \n",
        "    # ì„±ëŠ¥ ì €ì¥\n",
        "    avg_smape = np.mean(fold_scores)\n",
        "    individual_model_scores[building_num] = avg_smape\n",
        "    \n",
        "    print(f\"   ğŸ† í‰ê·  SMAPE: {avg_smape:.4f}\")\n",
        "    \n",
        "    # ì§„í–‰ë¥  ì¶œë ¥ (10ê°œë§ˆë‹¤)\n",
        "    if building_num % 10 == 0:\n",
        "        progress = building_num / len(department_store_buildings) * 100\n",
        "        avg_score = np.mean(list(individual_model_scores.values()))\n",
        "        print(f\"   â³ ì§„í–‰ë¥ : {progress:.1f}% | í‰ê·  SMAPE: {avg_score:.4f}\")\n",
        "\n",
        "# ë°±í™”ì  ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ê³„ì‚° (NaN ê°’ ì œê±°)\n",
        "dept_y_true_individual = Y[dept_mask_train][\"power_consumption\"].values\n",
        "dept_y_pred_individual = individual_model_oof[\"pred\"].values\n",
        "\n",
        "# NaN ê°’ ì œê±°\n",
        "valid_mask_individual = ~(pd.isna(dept_y_true_individual) | pd.isna(dept_y_pred_individual))\n",
        "if np.sum(valid_mask_individual) > 0:\n",
        "    total_individual_smape = smape(dept_y_true_individual[valid_mask_individual], dept_y_pred_individual[valid_mask_individual])\n",
        "else:\n",
        "    total_individual_smape = float('nan')\n",
        "\n",
        "print(f\"\\nğŸ¯ ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ ì „ì²´ SMAPE: {total_individual_smape:.4f}\")\n",
        "print(\"âœ… ê±´ë¬¼ë³„ ê°œë³„ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. ë°±í™”ì  ì•™ìƒë¸” ëª¨ë¸ + ê¸°ì¡´ ê²°ê³¼ ê²°í•©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¯ ë°±í™”ì  ì•™ìƒë¸” ëª¨ë¸ + ê¸°ì¡´ ê²°ê³¼ ê²°í•© ì¤‘...\n",
            "==================================================\n",
            "ğŸ“‚ ê¸°ì¡´ ì•™ìƒë¸” ê²°ê³¼ ë¡œë“œ ì¤‘...\n",
            "âœ… ê¸°ì¡´ ì œì¶œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: (16800, 2)\n",
            "ğŸ¢ ë°±í™”ì  ê±´ë¬¼ ë²ˆí˜¸: [18, 19, 27, 29, 32, 34, 40, 45, 54, 59, 63, 73, 74, 79, 88, 95]\n",
            "\n",
            "ğŸ“Š ë°±í™”ì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜:\n",
            "   ê°œë³„ ê±´ë¬¼ ëª¨ë¸: 70.0%\n",
            "   ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸: 30.0%\n",
            "ğŸ“Š ì•™ìƒë¸” ì „ ë°ì´í„° ìƒíƒœ í™•ì¸:\n",
            "   ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ ê°œìˆ˜: 2688\n",
            "   íƒ€ì…ë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ ê°œìˆ˜: 2688\n",
            "   ê°œë³„ ëª¨ë¸ OOF ì˜ˆì¸¡ê°’ ê°œìˆ˜: 32636\n",
            "   íƒ€ì…ë³„ ëª¨ë¸ OOF ì˜ˆì¸¡ê°’ ê°œìˆ˜: 32636\n",
            "   ì•™ìƒë¸” í›„ ê²€ì¦ ì˜ˆì¸¡ê°’ ê°œìˆ˜: 32636\n",
            "   ì•™ìƒë¸” í›„ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ ê°œìˆ˜: 2688\n",
            "ğŸ“Š ë°±í™”ì  í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ë±ìŠ¤ ìˆ˜: 2688\n",
            "ğŸ“Š ì˜ˆì¸¡ê°’ êµì²´ ê³¼ì •:\n",
            "   ë°±í™”ì  í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ë²”ìœ„: 2856 ~ 15959\n",
            "   ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ì¸ë±ìŠ¤ ë²”ìœ„: 2856 ~ 15959\n",
            "   ì„±ê³µì ìœ¼ë¡œ êµì²´ëœ ì˜ˆì¸¡ê°’: 2688ê°œ\n",
            "\n",
            "ğŸ“Š ì„±ëŠ¥ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° í™•ì¸:\n",
            "   ë°±í™”ì  í›ˆë ¨ ì¸ë±ìŠ¤ ìˆ˜: 32636\n",
            "   ì‹¤ì œê°’ ê°œìˆ˜: 32636\n",
            "   ì˜ˆì¸¡ê°’ ê°œìˆ˜: 32636\n",
            "   ìœ íš¨í•œ ë°ì´í„° ê°œìˆ˜: 32636\n",
            "\n",
            "ğŸ† ë°±í™”ì  ì„±ëŠ¥ ë¹„êµ:\n",
            "   ë°±í™”ì  íƒ€ì…ë³„ ëª¨ë¸ SMAPE: 3.6176\n",
            "   ë°±í™”ì  ê°œë³„ ëª¨ë¸ SMAPE: 3.4604\n",
            "   ë°±í™”ì  ì•™ìƒë¸” ëª¨ë¸ SMAPE: 3.3549\n",
            "\n",
            "ğŸ“ˆ ë°±í™”ì  ì„±ëŠ¥ í–¥ìƒ:\n",
            "   vs íƒ€ì…ë³„ ëª¨ë¸: +0.2628 SMAPE\n",
            "   vs ê°œë³„ ëª¨ë¸: +0.1055 SMAPE\n",
            "\n",
            "ğŸ‰ ë°±í™”ì  ì•™ìƒë¸”ì´ ê°œë³„ ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!\n",
            "\n",
            "âœ… ë°±í™”ì  ì•™ìƒë¸” + ê¸°ì¡´ ê²°ê³¼ ê²°í•© ì™„ë£Œ\n",
            "ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼:\n",
            "   - ë°±í™”ì  ê±´ë¬¼: ìƒˆë¡œìš´ ì•™ìƒë¸” ì˜ˆì¸¡ê°’ (2688ê°œ)\n",
            "   - ê¸°íƒ€ ê±´ë¬¼: ê¸°ì¡´ ì•™ìƒë¸” ì˜ˆì¸¡ê°’ (14112ê°œ)\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ¯ ë°±í™”ì  ì•™ìƒë¸” ëª¨ë¸ + ê¸°ì¡´ ê²°ê³¼ ê²°í•© ì¤‘...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. ê¸°ì¡´ ì•™ìƒë¸” ê²°ê³¼ ë¡œë“œ\n",
        "print(\"ğŸ“‚ ê¸°ì¡´ ì•™ìƒë¸” ê²°ê³¼ ë¡œë“œ ì¤‘...\")\n",
        "existing_submission = pd.read_csv('result_csv/ensemble_submission.csv')\n",
        "print(f\"âœ… ê¸°ì¡´ ì œì¶œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {existing_submission.shape}\")\n",
        "\n",
        "# 2. ë°±í™”ì  ê±´ë¬¼ ë²ˆí˜¸ ì¶”ì¶œ\n",
        "department_store_buildings = building_info[building_info['building_type'] == 'Department Store']['building_number'].tolist()\n",
        "print(f\"ğŸ¢ ë°±í™”ì  ê±´ë¬¼ ë²ˆí˜¸: {sorted(department_store_buildings)}\")\n",
        "\n",
        "# 3. ë°±í™”ì ì— ëŒ€í•œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì •\n",
        "individual_weight = 0.7  # ê°œë³„ ê±´ë¬¼ ëª¨ë¸ ê°€ì¤‘ì¹˜\n",
        "type_weight = 0.3        # ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜\n",
        "\n",
        "print(f\"\\nğŸ“Š ë°±í™”ì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜:\")\n",
        "print(f\"   ê°œë³„ ê±´ë¬¼ ëª¨ë¸: {individual_weight * 100}%\")\n",
        "print(f\"   ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸: {type_weight * 100}%\")\n",
        "\n",
        "# 4. ë°±í™”ì  ê±´ë¬¼ì— ëŒ€í•´ì„œë§Œ ì•™ìƒë¸” ìˆ˜í–‰\n",
        "department_store_mask = test_X['building_number'].isin(department_store_buildings)\n",
        "department_train_mask = X['building_number'].isin(department_store_buildings)\n",
        "\n",
        "# ë°ì´í„° ìƒíƒœ í™•ì¸\n",
        "print(f\"ğŸ“Š ì•™ìƒë¸” ì „ ë°ì´í„° ìƒíƒœ í™•ì¸:\")\n",
        "print(f\"   ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(individual_model_predictions)}\")\n",
        "print(f\"   íƒ€ì…ë³„ ëª¨ë¸ ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(type_model_predictions)}\")\n",
        "print(f\"   ê°œë³„ ëª¨ë¸ OOF ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(individual_model_oof)}\")\n",
        "print(f\"   íƒ€ì…ë³„ ëª¨ë¸ OOF ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(type_model_oof)}\")\n",
        "\n",
        "# ë°±í™”ì  ê²€ì¦ ë°ì´í„° ì•™ìƒë¸” (ì„±ëŠ¥ í‰ê°€ìš©)\n",
        "ensemble_oof_dept = (\n",
        "    individual_model_oof[\"pred\"] * individual_weight + \n",
        "    type_model_oof[\"pred\"] * type_weight\n",
        ")\n",
        "\n",
        "# ë°±í™”ì  í…ŒìŠ¤íŠ¸ ë°ì´í„° ì•™ìƒë¸”\n",
        "ensemble_predictions_dept = (\n",
        "    individual_model_predictions[\"answer\"] * individual_weight + \n",
        "    type_model_predictions[\"answer\"] * type_weight\n",
        ")\n",
        "\n",
        "print(f\"   ì•™ìƒë¸” í›„ ê²€ì¦ ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(ensemble_oof_dept)}\")\n",
        "print(f\"   ì•™ìƒë¸” í›„ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(ensemble_predictions_dept)}\")\n",
        "\n",
        "# 5. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± (ë°±í™”ì ì€ ìƒˆë¡œìš´ ê°’, ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ ê°’)\n",
        "final_submission = existing_submission.copy()\n",
        "\n",
        "# ë°±í™”ì  ê±´ë¬¼ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "dept_test_indices = test_X[department_store_mask].index\n",
        "print(f\"ğŸ“Š ë°±í™”ì  í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ë±ìŠ¤ ìˆ˜: {len(dept_test_indices)}\")\n",
        "\n",
        "# ë°±í™”ì  ê±´ë¬¼ ì˜ˆì¸¡ê°’ì„ ìƒˆë¡œìš´ ì•™ìƒë¸” ê²°ê³¼ë¡œ êµì²´\n",
        "print(f\"ğŸ“Š ì˜ˆì¸¡ê°’ êµì²´ ê³¼ì •:\")\n",
        "print(f\"   ë°±í™”ì  í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ë²”ìœ„: {dept_test_indices.min()} ~ {dept_test_indices.max()}\")\n",
        "print(f\"   ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ì¸ë±ìŠ¤ ë²”ìœ„: {ensemble_predictions_dept.index.min()} ~ {ensemble_predictions_dept.index.max()}\")\n",
        "\n",
        "replacement_count = 0\n",
        "for idx in dept_test_indices:\n",
        "    if idx < len(final_submission) and idx in ensemble_predictions_dept.index:\n",
        "        if pd.notna(ensemble_predictions_dept.loc[idx]):\n",
        "            final_submission.loc[idx, 'answer'] = ensemble_predictions_dept.loc[idx]\n",
        "            replacement_count += 1\n",
        "        else:\n",
        "            print(f\"   âš ï¸ ì¸ë±ìŠ¤ {idx}ì˜ ì˜ˆì¸¡ê°’ì´ NaNì…ë‹ˆë‹¤.\")\n",
        "    else:\n",
        "        print(f\"   âš ï¸ ì¸ë±ìŠ¤ {idx}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"   ì„±ê³µì ìœ¼ë¡œ êµì²´ëœ ì˜ˆì¸¡ê°’: {replacement_count}ê°œ\")\n",
        "\n",
        "# 6. ë°±í™”ì  ì„±ëŠ¥ ê³„ì‚°\n",
        "# ë°±í™”ì  í›ˆë ¨ ë°ì´í„°ì˜ ì‹¤ì œê°’ê³¼ ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ì¶”ì¶œ\n",
        "dept_train_indices = X[department_train_mask].index\n",
        "dept_y_true = Y.loc[dept_train_indices, \"power_consumption\"].values\n",
        "dept_y_pred = ensemble_oof_dept.values\n",
        "\n",
        "print(f\"\\nğŸ“Š ì„±ëŠ¥ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° í™•ì¸:\")\n",
        "print(f\"   ë°±í™”ì  í›ˆë ¨ ì¸ë±ìŠ¤ ìˆ˜: {len(dept_train_indices)}\")\n",
        "print(f\"   ì‹¤ì œê°’ ê°œìˆ˜: {len(dept_y_true)}\")\n",
        "print(f\"   ì˜ˆì¸¡ê°’ ê°œìˆ˜: {len(dept_y_pred)}\")\n",
        "\n",
        "# NaN ê°’ ì œê±°\n",
        "valid_mask = ~(pd.isna(dept_y_true) | pd.isna(dept_y_pred))\n",
        "print(f\"   ìœ íš¨í•œ ë°ì´í„° ê°œìˆ˜: {np.sum(valid_mask)}\")\n",
        "\n",
        "if np.sum(valid_mask) > 0:\n",
        "    dept_y_true_clean = dept_y_true[valid_mask]\n",
        "    dept_y_pred_clean = dept_y_pred[valid_mask]\n",
        "    \n",
        "    ensemble_smape_dept = smape(dept_y_true_clean, dept_y_pred_clean)\n",
        "    \n",
        "    print(f\"\\nğŸ† ë°±í™”ì  ì„±ëŠ¥ ë¹„êµ:\")\n",
        "    if not np.isnan(total_type_smape):\n",
        "        print(f\"   ë°±í™”ì  íƒ€ì…ë³„ ëª¨ë¸ SMAPE: {total_type_smape:.4f}\")\n",
        "    else:\n",
        "        print(f\"   ë°±í™”ì  íƒ€ì…ë³„ ëª¨ë¸ SMAPE: ê³„ì‚° ë¶ˆê°€\")\n",
        "        \n",
        "    if not np.isnan(total_individual_smape):\n",
        "        print(f\"   ë°±í™”ì  ê°œë³„ ëª¨ë¸ SMAPE: {total_individual_smape:.4f}\")\n",
        "    else:\n",
        "        print(f\"   ë°±í™”ì  ê°œë³„ ëª¨ë¸ SMAPE: ê³„ì‚° ë¶ˆê°€\")\n",
        "        \n",
        "    print(f\"   ë°±í™”ì  ì•™ìƒë¸” ëª¨ë¸ SMAPE: {ensemble_smape_dept:.4f}\")\n",
        "    \n",
        "    # ì„±ëŠ¥ í–¥ìƒ í™•ì¸ (NaNì´ ì•„ë‹Œ ê²½ìš°ë§Œ)\n",
        "    if not np.isnan(total_type_smape) and not np.isnan(total_individual_smape):\n",
        "        improvement_vs_type = total_type_smape - ensemble_smape_dept\n",
        "        improvement_vs_individual = total_individual_smape - ensemble_smape_dept\n",
        "        \n",
        "        print(f\"\\nğŸ“ˆ ë°±í™”ì  ì„±ëŠ¥ í–¥ìƒ:\")\n",
        "        print(f\"   vs íƒ€ì…ë³„ ëª¨ë¸: {improvement_vs_type:+.4f} SMAPE\")\n",
        "        print(f\"   vs ê°œë³„ ëª¨ë¸: {improvement_vs_individual:+.4f} SMAPE\")\n",
        "        \n",
        "        if ensemble_smape_dept < min(total_type_smape, total_individual_smape):\n",
        "            print(\"\\nğŸ‰ ë°±í™”ì  ì•™ìƒë¸”ì´ ê°œë³„ ëª¨ë¸ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!\")\n",
        "        else:\n",
        "            print(\"\\nâš ï¸ ë°±í™”ì  ì•™ìƒë¸” ì„±ëŠ¥ì´ ê¸°ëŒ€ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.\")\n",
        "else:\n",
        "    print(\"âš ï¸ ë°±í™”ì  ì„±ëŠ¥ ê³„ì‚° ì¤‘ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    ensemble_smape_dept = float('nan')\n",
        "\n",
        "print(f\"\\nâœ… ë°±í™”ì  ì•™ìƒë¸” + ê¸°ì¡´ ê²°ê³¼ ê²°í•© ì™„ë£Œ\")\n",
        "print(f\"ğŸ“Š ìµœì¢… ì œì¶œ íŒŒì¼:\")\n",
        "print(f\"   - ë°±í™”ì  ê±´ë¬¼: ìƒˆë¡œìš´ ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ({len(dept_test_indices)}ê°œ)\")\n",
        "print(f\"   - ê¸°íƒ€ ê±´ë¬¼: ê¸°ì¡´ ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ({len(final_submission) - len(dept_test_indices)}ê°œ)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ë° ì €ì¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\n",
            "âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥: result_csv/ensemble_submission_with_dept_holiday.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ìŒìˆ˜ê°’ ì²˜ë¦¬ (ì „ë ¥ ì†Œë¹„ëŸ‰ì€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ìŒ)\n",
        "final_submission[\"answer\"] = np.maximum(final_submission[\"answer\"], 0)\n",
        "\n",
        "# ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "final_submission.to_csv('result_csv/ensemble_submission_with_dept_holiday3.csv', index=False)\n",
        "\n",
        "print(\"âœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥: result_csv/ensemble_submission_with_dept_holiday.csv\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sd2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
