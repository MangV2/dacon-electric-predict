{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ ModernTCN ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ì‹œì‘!\n",
            "ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
            "==================================================\n",
            "âœ… ModernTCN ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ModernTCN ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸\n",
        "# XGBoost ëŒ€ì‹  ModernTCN-short-term ì‚¬ìš©\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "import random as rn\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# ì‹œë“œ ì„¤ì •\n",
        "RANDOM_SEED = 2025\n",
        "np.random.seed(RANDOM_SEED)\n",
        "rn.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª ì „ì²´ ë°ì´í„° ê¸°ê°„ í™•ì¸...\n",
            "ğŸ“… ì „ì²´ train ë°ì´í„° ê¸°ê°„: 2024-06-01 00:00:00 ~ 2024-08-24 23:00:00\n",
            "ğŸ“Š ì „ì²´ train ë°ì´í„° ìˆ˜: 204000ê°œ\n",
            "ğŸ“Š ì´ ê¸°ê°„: 84ì¼\n",
            "ğŸ“Š ì´ ì£¼ì°¨: 12ì£¼\n",
            "\\nğŸ“Š ê±´ë¬¼ 1 ë°ì´í„°: 2040ê°œ\n",
            "ğŸ“… ê±´ë¬¼ 1 ê¸°ê°„: 2024-06-01 00:00:00 ~ 2024-08-24 23:00:00\n",
            "ğŸ“Š ê±´ë¬¼ 1 ì´ ê¸°ê°„: 84ì¼\n",
            "ğŸ“Š ê±´ë¬¼ 1 ì´ ì£¼ì°¨: 12ì£¼\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²´ ë°ì´í„° ê¸°ê°„ í™•ì¸\n",
        "print(\"ğŸ§ª ì „ì²´ ë°ì´í„° ê¸°ê°„ í™•ì¸...\")\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "train = pd.read_csv('data/train.csv')\n",
        "train = train.rename(columns={'ì¼ì‹œ': 'date_time'})\n",
        "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
        "\n",
        "print(f\"ğŸ“… ì „ì²´ train ë°ì´í„° ê¸°ê°„: {train['date_time'].min()} ~ {train['date_time'].max()}\")\n",
        "print(f\"ğŸ“Š ì „ì²´ train ë°ì´í„° ìˆ˜: {len(train)}ê°œ\")\n",
        "print(f\"ğŸ“Š ì´ ê¸°ê°„: {(train['date_time'].max() - train['date_time'].min()).days}ì¼\")\n",
        "print(f\"ğŸ“Š ì´ ì£¼ì°¨: {(train['date_time'].max() - train['date_time'].min()).days // 7}ì£¼\")\n",
        "\n",
        "# ê±´ë¬¼ 1ê°œì˜ ì „ì²´ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸\n",
        "building_1_data = train[train['ê±´ë¬¼ë²ˆí˜¸'] == 1].copy()\n",
        "print(f\"\\\\nğŸ“Š ê±´ë¬¼ 1 ë°ì´í„°: {len(building_1_data)}ê°œ\")\n",
        "print(f\"ğŸ“… ê±´ë¬¼ 1 ê¸°ê°„: {building_1_data['date_time'].min()} ~ {building_1_data['date_time'].max()}\")\n",
        "print(f\"ğŸ“Š ê±´ë¬¼ 1 ì´ ê¸°ê°„: {(building_1_data['date_time'].max() - building_1_data['date_time'].min()).days}ì¼\")\n",
        "print(f\"ğŸ“Š ê±´ë¬¼ 1 ì´ ì£¼ì°¨: {(building_1_data['date_time'].max() - building_1_data['date_time'].min()).days // 7}ì£¼\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. í‰ê°€ í•¨ìˆ˜ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "def smape(gt, preds):\n",
        "    \"\"\"SMAPE (Symmetric Mean Absolute Percentage Error) ê³„ì‚°\"\"\"\n",
        "    gt = np.array(gt)\n",
        "    preds = np.array(preds)\n",
        "    v = 2 * abs(preds - gt) / (abs(preds) + abs(gt))\n",
        "    score = np.mean(v) * 100\n",
        "    return score\n",
        "    \n",
        "def weighted_mse(alpha=1):\n",
        "    \"\"\"ê°€ì¤‘ MSE ì†ì‹¤ í•¨ìˆ˜ (Under-predictionì— ë” í° í˜ë„í‹°)\"\"\"\n",
        "    def weighted_mse_fixed(label, pred):\n",
        "        residual = (label - pred).astype(\"float\")\n",
        "        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n",
        "        hess = np.where(residual > 0, 2 * alpha, 2.0)\n",
        "        return grad, hess\n",
        "    return weighted_mse_fixed\n",
        "\n",
        "def custom_smape(preds, dtrain):\n",
        "    \"\"\"XGBoostìš© SMAPE í‰ê°€ í•¨ìˆ˜\"\"\"\n",
        "    labels = dtrain.get_label()\n",
        "    return 'custom_smape', np.mean(2 * abs(preds - labels) / (abs(preds) + abs(labels))) * 100\n",
        "\n",
        "print(\"âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\n",
            "âœ… Train ë°ì´í„°: (204000, 10)\n",
            "âœ… Test ë°ì´í„°: (16800, 7)\n",
            "âœ… Building info: (100, 7)\n",
            "âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "building_info = pd.read_csv('data/building_info.csv')\n",
        "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
        "\n",
        "print(f\"âœ… Train ë°ì´í„°: {train.shape}\")\n",
        "print(f\"âœ… Test ë°ì´í„°: {test.shape}\")\n",
        "print(f\"âœ… Building info: {building_info.shape}\")\n",
        "\n",
        "# ì»¬ëŸ¼ëª… ì˜ì–´ë¡œ ë³€ê²½ (ì‘ë…„ ìˆ˜ìƒì ë°©ì‹)\n",
        "train = train.rename(columns={\n",
        "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
        "    'ì¼ì‹œ': 'date_time',\n",
        "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
        "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
        "    'í’ì†(m/s)': 'windspeed',\n",
        "    'ìŠµë„(%)': 'humidity',\n",
        "    'ì¼ì¡°(hr)': 'sunshine',\n",
        "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation',\n",
        "    'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'power_consumption'\n",
        "})\n",
        "train.drop('num_date_time', axis=1, inplace=True)\n",
        "\n",
        "test = test.rename(columns={\n",
        "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
        "    'ì¼ì‹œ': 'date_time',\n",
        "    'ê¸°ì˜¨(Â°C)': 'temperature',\n",
        "    'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',\n",
        "    'í’ì†(m/s)': 'windspeed',\n",
        "    'ìŠµë„(%)': 'humidity',\n",
        "    'ì¼ì¡°(hr)': 'sunshine',\n",
        "    'ì¼ì‚¬(MJ/m2)': 'solar_radiation'\n",
        "})\n",
        "test.drop('num_date_time', axis=1, inplace=True)\n",
        "\n",
        "building_info = building_info.rename(columns={\n",
        "    'ê±´ë¬¼ë²ˆí˜¸': 'building_number',\n",
        "    'ê±´ë¬¼ìœ í˜•': 'building_type',\n",
        "    'ì—°ë©´ì (m2)': 'total_area',\n",
        "    'ëƒ‰ë°©ë©´ì (m2)': 'cooling_area',\n",
        "    'íƒœì–‘ê´‘ìš©ëŸ‰(kW)': 'solar_power_capacity',\n",
        "    'ESSì €ì¥ìš©ëŸ‰(kWh)': 'ess_capacity',\n",
        "    'PCSìš©ëŸ‰(kW)': 'pcs_capacity'\n",
        "})\n",
        "\n",
        "# ê±´ë¬¼ ìœ í˜• ì˜ì–´ë¡œ ë²ˆì—­\n",
        "translation_dict = {\n",
        "    'ê±´ë¬¼ê¸°íƒ€': 'Other Buildings',\n",
        "    'ê³µê³µ': 'Public',\n",
        "    'í•™êµ': 'University',\n",
        "    'ë°±í™”ì ': 'Department Store',\n",
        "    'ë³‘ì›': 'Hospital',\n",
        "    'ìƒìš©': 'Commercial',\n",
        "    'ì•„íŒŒíŠ¸': 'Apartment',\n",
        "    'ì—°êµ¬ì†Œ': 'Research Institute',\n",
        "    'IDC(ì „í™”êµ­)': 'IDC',\n",
        "    'í˜¸í…”': 'Hotel'\n",
        "}\n",
        "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
        "\n",
        "# íƒœì–‘ê´‘/ESS ì„¤ë¹„ ìœ ë¬´ í”¼ì²˜ ìƒì„±\n",
        "building_info['solar_power_utility'] = np.where(building_info.solar_power_capacity != '-', 1, 0)\n",
        "building_info['ess_utility'] = np.where(building_info.ess_capacity != '-', 1, 0)\n",
        "\n",
        "# ê±´ë¬¼ ì •ë³´ ë³‘í•©\n",
        "train = pd.merge(train, building_info, on='building_number', how='left')\n",
        "test = pd.merge(test, building_info, on='building_number', how='left')\n",
        "\n",
        "print(\"âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Feature Engineering (ì‘ë…„ ìˆ˜ìƒì ë°©ì‹)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Feature Engineering ì‹œì‘...\n",
            "âœ… ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ë‚ ì§œ/ì‹œê°„ ë³€í™˜ ë° ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„±\n",
        "print(\"ğŸ”§ Feature Engineering ì‹œì‘...\")\n",
        "\n",
        "train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
        "test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
        "\n",
        "# ê¸°ë³¸ ì‹œê°„ í”¼ì²˜\n",
        "for df in [train, test]:\n",
        "    df['hour'] = df['date_time'].dt.hour\n",
        "    df['day'] = df['date_time'].dt.day\n",
        "    df['month'] = df['date_time'].dt.month\n",
        "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
        "\n",
        "print(\"âœ… ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„±\n",
        "def calculate_day_values(dataframe, target_column, output_column, aggregation_func):\n",
        "    \"\"\"ì¼ë³„ í†µê³„ê°’ ê³„ì‚° í•¨ìˆ˜\"\"\"\n",
        "    result_dict = {}\n",
        "    grouped_temp = dataframe.groupby(['building_number', 'month', 'day'])[target_column].agg(aggregation_func)\n",
        "    \n",
        "    for (building, month, day), value in grouped_temp.items():\n",
        "        result_dict.setdefault(building, {}).setdefault(month, {})[day] = value\n",
        "    \n",
        "    dataframe[output_column] = [\n",
        "        result_dict.get(row['building_number'], {}).get(row['month'], {}).get(row['day'], None)\n",
        "        for _, row in dataframe.iterrows()\n",
        "    ]\n",
        "\n",
        "# ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„±\n",
        "for df in [train, test]:\n",
        "    calculate_day_values(df, 'temperature', 'day_max_temperature', 'max')\n",
        "    calculate_day_values(df, 'temperature', 'day_mean_temperature', 'mean')\n",
        "    calculate_day_values(df, 'temperature', 'day_min_temperature', 'min')\n",
        "    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']\n",
        "\n",
        "print(\"âœ… ì¼ë³„ ì˜¨ë„ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì œê±°í•  ì´ìƒì¹˜ ê°œìˆ˜: 68\n",
            "ë‚¨ì€ í–‰ ê°œìˆ˜: 203932\n"
          ]
        }
      ],
      "source": [
        "# ì´ìƒì¹˜ ì œê±° ë° ì¶”ê°€ í”¼ì²˜ ìƒì„± (0 ì œê±°)\n",
        "outlier_idx = train.index[train['power_consumption'] == 0].tolist()\n",
        "print(f\"ì œê±°í•  ì´ìƒì¹˜ ê°œìˆ˜: {len(outlier_idx)}\")\n",
        "train.drop(index=outlier_idx, inplace=True)\n",
        "print(f\"ë‚¨ì€ í–‰ ê°œìˆ˜: {train.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ï¿½ï¿½ Z-score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€ (ì„ê³„ê°’: 4.5)\n",
            "============================================================\n",
            "ğŸ“Š ë°œê²¬ëœ ì´ìƒì¹˜: 68ê°œ\n",
            "ï¿½ï¿½ ì´ìƒì¹˜ê°€ ìˆëŠ” ê±´ë¬¼ ìˆ˜: 11ê°œ\n",
            "ğŸ¢ ì´ìƒì¹˜ê°€ ìˆëŠ” ê±´ë¬¼ ë²ˆí˜¸: [23, 30, 41, 43, 52, 64, 67, 72, 76, 81, 99]\n",
            "\n",
            "ğŸ“ˆ ê±´ë¬¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜:\n",
            "   ê±´ë¬¼ 67 (IDC): 48ê°œ\n",
            "   ê±´ë¬¼ 41 (Commercial): 4ê°œ\n",
            "   ê±´ë¬¼ 43 (IDC): 4ê°œ\n",
            "   ê±´ë¬¼ 30 (IDC): 2ê°œ\n",
            "   ê±´ë¬¼ 52 (IDC): 2ê°œ\n",
            "   ê±´ë¬¼ 76 (Commercial): 2ê°œ\n",
            "   ê±´ë¬¼ 99 (Commercial): 2ê°œ\n",
            "   ê±´ë¬¼ 23 (Research Institute): 1ê°œ\n",
            "   ê±´ë¬¼ 64 (IDC): 1ê°œ\n",
            "   ê±´ë¬¼ 72 (Public): 1ê°œ\n",
            "   ê±´ë¬¼ 81 (IDC): 1ê°œ\n",
            "\n",
            "ğŸ“Š ì´ìƒì¹˜ ìƒì„¸ ë¶„ì„:\n",
            "============================================================\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 23 (Research Institute):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 1ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 9324.00 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 9324.00 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 9324.00 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 2339.77 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.05%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 30 (IDC):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 2ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 2444.40 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 7374.24 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 4909.32 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 9801.04 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.10%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 41 (Commercial):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 4ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 246.24 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 2102.94 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 1373.76 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 2710.89 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.20%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 43 (IDC):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 4ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 834.60 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 9765.60 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 5587.50 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 14058.80 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.20%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 52 (IDC):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 2ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 1437.84 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 2605.68 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 2021.76 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 5099.60 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.10%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 64 (IDC):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 1ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 13615.92 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 13615.92 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 13615.92 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 11002.92 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.05%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 67 (IDC):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 48ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 60.30 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 6263.10 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 5541.75 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 10722.11 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 2.35%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 72 (Public):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 1ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 39.00 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 39.00 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 39.00 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 1395.94 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.05%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 76 (Commercial):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 2ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 1406.16 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 1595.52 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 1500.84 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 3120.70 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.10%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 81 (IDC):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 1ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 650.64 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 650.64 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 650.64 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 1223.75 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.05%\n",
            "\n",
            "ğŸ¢ ê±´ë¬¼ 99 (Commercial):\n",
            "   ì´ìƒì¹˜ ê°œìˆ˜: 2ê°œ\n",
            "   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: 370.80 kWh\n",
            "   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: 380.88 kWh\n",
            "   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: 375.84 kWh\n",
            "   ì „ì²´ ë°ì´í„° í‰ê· : 1072.62 kWh\n",
            "   ì´ìƒì¹˜ ë¹„ìœ¨: 0.10%\n"
          ]
        }
      ],
      "source": [
        "# ì´ìƒì¹˜ ì œê±° (Z-score ê¸°ë°˜ )\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def detect_outliers_zscore_only(train, threshold=4.5):\n",
        "    \"\"\"\n",
        "    Z-score ë°©ë²•ìœ¼ë¡œë§Œ ì´ìƒì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        train (pd.DataFrame): ì „ì²˜ë¦¬ëœ ì „ì²´ í•™ìŠµ ë°ì´í„°\n",
        "        threshold (float): Z-score ì„ê³„ê°’ (ê¸°ë³¸ê°’: 3)\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: ì´ìƒì¹˜ ë°ì´í„°\n",
        "    \"\"\"\n",
        "    print(f\"ï¿½ï¿½ Z-score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€ (ì„ê³„ê°’: {threshold})\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    outliers_list = []\n",
        "    \n",
        "    for building_num in train['building_number'].unique():\n",
        "        building_data = train[train['building_number'] == building_num]['power_consumption']\n",
        "        \n",
        "        # Z-score ê³„ì‚°\n",
        "        z_scores = np.abs(stats.zscore(building_data))\n",
        "        \n",
        "        # ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ì´ìƒì¹˜ ì°¾ê¸°\n",
        "        building_outliers = train[(train['building_number'] == building_num) & \n",
        "                                          (z_scores > threshold)]\n",
        "        \n",
        "        if not building_outliers.empty:\n",
        "            outliers_list.append(building_outliers)\n",
        "    \n",
        "    outliers_df = pd.concat(outliers_list) if outliers_list else pd.DataFrame()\n",
        "    \n",
        "    print(f\"ğŸ“Š ë°œê²¬ëœ ì´ìƒì¹˜: {len(outliers_df)}ê°œ\")\n",
        "    \n",
        "    if not outliers_df.empty:\n",
        "        print(f\"ï¿½ï¿½ ì´ìƒì¹˜ê°€ ìˆëŠ” ê±´ë¬¼ ìˆ˜: {outliers_df['building_number'].nunique()}ê°œ\")\n",
        "        print(f\"ğŸ¢ ì´ìƒì¹˜ê°€ ìˆëŠ” ê±´ë¬¼ ë²ˆí˜¸: {sorted(outliers_df['building_number'].unique())}\")\n",
        "        \n",
        "        # ê±´ë¬¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜\n",
        "        building_counts = outliers_df['building_number'].value_counts()\n",
        "        print(f\"\\nğŸ“ˆ ê±´ë¬¼ë³„ ì´ìƒì¹˜ ê°œìˆ˜:\")\n",
        "        for building_num, count in building_counts.items():\n",
        "            building_type = building_info[building_info['building_number'] == building_num]['building_type'].iloc[0]\n",
        "            print(f\"   ê±´ë¬¼ {building_num} ({building_type}): {count}ê°œ\")\n",
        "    else:\n",
        "        print(\"âœ… ì´ìƒì¹˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    \n",
        "    return outliers_df\n",
        "\n",
        "# Z-score ì´ìƒì¹˜ íƒì§€ ì‹¤í–‰\n",
        "outliers_df = detect_outliers_zscore_only(train, threshold=4.5)\n",
        "\n",
        "# ì´ìƒì¹˜ ìƒì„¸ ë¶„ì„\n",
        "if not outliers_df.empty:\n",
        "    print(f\"\\nğŸ“Š ì´ìƒì¹˜ ìƒì„¸ ë¶„ì„:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for building_num in sorted(outliers_df['building_number'].unique()):\n",
        "        building_outliers = outliers_df[outliers_df['building_number'] == building_num]\n",
        "        building_type = building_info[building_info['building_number'] == building_num]['building_type'].iloc[0]\n",
        "        \n",
        "        print(f\"\\nğŸ¢ ê±´ë¬¼ {building_num} ({building_type}):\")\n",
        "        print(f\"   ì´ìƒì¹˜ ê°œìˆ˜: {len(building_outliers)}ê°œ\")\n",
        "        print(f\"   ìµœì†Œ ì „ë ¥ì†Œë¹„ëŸ‰: {building_outliers['power_consumption'].min():.2f} kWh\")\n",
        "        print(f\"   ìµœëŒ€ ì „ë ¥ì†Œë¹„ëŸ‰: {building_outliers['power_consumption'].max():.2f} kWh\")\n",
        "        print(f\"   í‰ê·  ì „ë ¥ì†Œë¹„ëŸ‰: {building_outliers['power_consumption'].mean():.2f} kWh\")\n",
        "        \n",
        "        # ì „ì²´ ê±´ë¬¼ ë°ì´í„°ì™€ ë¹„êµ\n",
        "        building_all = train[train['building_number'] == building_num]\n",
        "        print(f\"   ì „ì²´ ë°ì´í„° í‰ê· : {building_all['power_consumption'].mean():.2f} kWh\")\n",
        "        print(f\"   ì´ìƒì¹˜ ë¹„ìœ¨: {len(building_outliers)/len(building_all)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ì•ˆì „í•œ ì´ìƒì¹˜ ì œê±° ì‹œì‘\n",
            "   ì›ë³¸ ë°ì´í„° í¬ê¸°: 203932ê°œ\n",
            "   ì œê±°í•  ì´ìƒì¹˜ ê°œìˆ˜: 68ê°œ\n",
            "   ì œê±° í›„ ë°ì´í„° í¬ê¸°: 203864ê°œ\n",
            "   ì‹¤ì œ ì œê±°ëœ í–‰ ìˆ˜: 68ê°œ\n",
            "   ì œê±°ëœ ë°ì´í„° ë¹„ìœ¨: 0.03%\n",
            "\n",
            "ğŸ“Š ì•ˆì „í•œ ì´ìƒì¹˜ ì œê±° ê²°ê³¼:\n",
            "   í™•ì¸: 203864ê°œ\n",
            "\n",
            "âœ… train ë³€ìˆ˜ì— ì •ì œëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "def drop_outliers_safe(train, outliers_df):\n",
        "    \"\"\"\n",
        "    ì•ˆì „í•œ ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ ì œê±° (ëª¨ë“  ì»¬ëŸ¼ ë§¤ì¹­)\n",
        "    \n",
        "    Args:\n",
        "        train (pd.DataFrame): ì „ì²´ í•™ìŠµ ë°ì´í„°\n",
        "        outliers_df (pd.DataFrame): ì´ìƒì¹˜ ë°ì´í„°\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: ì´ìƒì¹˜ê°€ ì œê±°ëœ ë°ì´í„°\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ” ì•ˆì „í•œ ì´ìƒì¹˜ ì œê±° ì‹œì‘\")\n",
        "    print(f\"   ì›ë³¸ ë°ì´í„° í¬ê¸°: {len(train)}ê°œ\")\n",
        "    print(f\"   ì œê±°í•  ì´ìƒì¹˜ ê°œìˆ˜: {len(outliers_df)}ê°œ\")\n",
        "    \n",
        "    # ì •ì œëœ ë°ì´í„° ì´ˆê¸°í™”\n",
        "    cleaned_data = train.copy()\n",
        "    \n",
        "    # ì´ìƒì¹˜ê°€ ìˆëŠ” ê±´ë¬¼ë“¤ë§Œ ì²˜ë¦¬\n",
        "    outlier_buildings = outliers_df['building_number'].unique()\n",
        "    \n",
        "    removed_count = 0\n",
        "    for building_num in outlier_buildings:\n",
        "        building_outliers = outliers_df[outliers_df['building_number'] == building_num]\n",
        "        \n",
        "        # í•´ë‹¹ ê±´ë¬¼ì˜ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ ì œê±°\n",
        "        building_mask = cleaned_data['building_number'] == building_num\n",
        "        \n",
        "        for _, outlier_row in building_outliers.iterrows():\n",
        "            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í–‰ ì°¾ê¸°\n",
        "            match_mask = (\n",
        "                (cleaned_data['building_number'] == outlier_row['building_number']) &\n",
        "                (cleaned_data['date_time'] == outlier_row['date_time']) &\n",
        "                (cleaned_data['power_consumption'] == outlier_row['power_consumption'])\n",
        "            )\n",
        "            \n",
        "            # ì¼ì¹˜í•˜ëŠ” í–‰ ì œê±°\n",
        "            cleaned_data = cleaned_data[~match_mask]\n",
        "            removed_count += match_mask.sum()\n",
        "    \n",
        "    print(f\"   ì œê±° í›„ ë°ì´í„° í¬ê¸°: {len(cleaned_data)}ê°œ\")\n",
        "    print(f\"   ì‹¤ì œ ì œê±°ëœ í–‰ ìˆ˜: {removed_count}ê°œ\")\n",
        "    print(f\"   ì œê±°ëœ ë°ì´í„° ë¹„ìœ¨: {removed_count/len(train)*100:.2f}%\")\n",
        "    \n",
        "    return cleaned_data\n",
        "\n",
        "# ì•ˆì „í•œ ì´ìƒì¹˜ ì œê±° ì‹¤í–‰\n",
        "train = drop_outliers_safe(train, outliers_df)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "print(f\"\\nğŸ“Š ì•ˆì „í•œ ì´ìƒì¹˜ ì œê±° ê²°ê³¼:\")\n",
        "print(f\"   í™•ì¸: {len(train)}ê°œ\")\n",
        "\n",
        "# ì •ì œëœ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ ë³€ìˆ˜ì— ì €ì¥\n",
        "print(f\"\\nâœ… train ë³€ìˆ˜ì— ì •ì œëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê³µíœ´ì¼(holiday), ì£¼ë§(weekend), ë‹«ì€ë‚ (close)ë¡œ ì„¸ë¶„í™”# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. ê¸°ë³¸ ê³µíœ´ì¼ ë° ì£¼ë§ í”¼ì²˜ ìƒì„±\n",
        "holi_weekday = ['2024-06-06', '2024-08-15']  # ê³µíœ´ì¼ ëª©ë¡\n",
        "for df in [train, test]:\n",
        "    # ê³µíœ´ì¼ (êµ­ê°€ ê³µíœ´ì¼)\n",
        "    df['holiday'] = np.where(\n",
        "        df.date_time.dt.strftime('%Y-%m-%d').isin(holi_weekday), 1, 0\n",
        "    )\n",
        "    \n",
        "    # ì£¼ë§ (í† ìš”ì¼, ì¼ìš”ì¼)\n",
        "    df['weekend'] = np.where(\n",
        "        df.day_of_week >= 5, 1, 0\n",
        "    )\n",
        "    \n",
        "    # ë‹«ì€ë‚  (ì´ˆê¸°ê°’ 0ìœ¼ë¡œ ì„¤ì •, ë‚˜ì¤‘ì— ê±´ë¬¼ë³„ ê·œì¹™ ì ìš©)\n",
        "    df['close'] = 0\n",
        "\n",
        "\n",
        "# ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„± (Cyclical Features)\n",
        "for df in [train, test]:\n",
        "    # ì‹œê°„ ì£¼ê¸°ì„±\n",
        "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 23.0)\n",
        "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 23.0)\n",
        "    \n",
        "    # ë‚ ì§œ ì£¼ê¸°ì„±\n",
        "    df['sin_date'] = -np.sin(2 * np.pi * (df['month'] + df['day'] / 31) / 12)\n",
        "    df['cos_date'] = -np.cos(2 * np.pi * (df['month'] + df['day'] / 31) / 12)\n",
        "    \n",
        "    # ì›” ì£¼ê¸°ì„±\n",
        "    df['sin_month'] = -np.sin(2 * np.pi * df['month'] / 12.0)\n",
        "    df['cos_month'] = -np.cos(2 * np.pi * df['month'] / 12.0)\n",
        "    \n",
        "    # ìš”ì¼ ì£¼ê¸°ì„±\n",
        "    df['sin_dayofweek'] = -np.sin(2 * np.pi * (df['day_of_week'] + 1) / 7.0)\n",
        "    df['cos_dayofweek'] = -np.cos(2 * np.pi * (df['day_of_week'] + 1) / 7.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_specific_building_holidays(df):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ê±´ë¬¼ë“¤ì˜ íœ´ì¼ ê·œì¹™ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜\n",
        "    - 18ë²ˆ ê±´ë¬¼: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (close)\n",
        "    - 27, 40, 59, 63ë²ˆ ê±´ë¬¼: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (close)\n",
        "    - 29ë²ˆ ê±´ë¬¼: ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (close)\n",
        "    - 32ë²ˆ ê±´ë¬¼: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ë¬´ (close)\n",
        "    \"\"\"\n",
        "    print(\"ğŸ”§ íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì¤‘... (ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨)\")\n",
        "    department_store_buildings = building_info[building_info['building_type'] == 'Department Store']['building_number'].tolist()\n",
        "\n",
        "    # ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ (18ë²ˆ)\n",
        "    every_sunday_buildings = [18]\n",
        "\n",
        "    # í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ë“¤ (27, 40, 59, 63)\n",
        "    odd_week_sunday_buildings = [27, 40, 59, 63]\n",
        "\n",
        "    # ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ (29)\n",
        "    special_holiday_building = [29]\n",
        "\n",
        "    # í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ë¬´ ê±´ë¬¼ (32)\n",
        "    odd_week_monday_buildings = [32]\n",
        "\n",
        "    # ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚° (ì¼ìš”ì¼ ì‹œì‘)\n",
        "    def get_calendar_week_of_month(date_series):\n",
        "        \"\"\"ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚° (ì›”ì˜ ì²«ë‚ ì´ í¬í•¨ëœ ì£¼ë¥¼ 1ì£¼ì°¨ë¡œ)\"\"\"\n",
        "        result = []\n",
        "        for date in date_series:\n",
        "            first_day = date.replace(day=1)\n",
        "            first_day_weekday = first_day.weekday()\n",
        "            days_to_week_start = (first_day_weekday + 1) % 7  # ì¼ìš”ì¼ ê¸°ì¤€\n",
        "            week_start_of_first = first_day - pd.Timedelta(days=days_to_week_start)\n",
        "            days_since_first_week_start = (date - week_start_of_first).days\n",
        "            week_num = (days_since_first_week_start // 7) + 1\n",
        "            result.append(week_num)\n",
        "        return result\n",
        "\n",
        "    # ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚°\n",
        "    df['calendar_week_of_month'] = get_calendar_week_of_month(df['date_time'])\n",
        "\n",
        "    # 0. ë§¤ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (18ë²ˆ ê±´ë¬¼) - closeì— ì ìš©\n",
        "    for building_num in every_sunday_buildings:\n",
        "        mask = (df['building_number'] == building_num) & (df['day_of_week'] == 6)\n",
        "        df.loc[mask, 'close'] = 1\n",
        "        count = mask.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ì¼ {count}ê°œ ì ìš©\")\n",
        "\n",
        "    # 1. í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (27, 40, 59, 63ë²ˆ ê±´ë¬¼) - closeì— ì ìš©\n",
        "    for building_num in odd_week_sunday_buildings:\n",
        "        mask = (df['building_number'] == building_num) & \\\n",
        "               (df['day_of_week'] == 6) & \\\n",
        "               (df['calendar_week_of_month'] % 2 == 1)\n",
        "        df.loc[mask, 'close'] = 1\n",
        "        count = mask.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ {count}ê°œ ì ìš©\")\n",
        "\n",
        "    # 2. ë§¤ë‹¬ 10ì¼ + 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ íœ´ë¬´ (29ë²ˆ ê±´ë¬¼) - closeì— ì ìš©\n",
        "    for building_num in special_holiday_building:\n",
        "        mask_10th = (df['building_number'] == building_num) & (df['date_time'].dt.day == 10)\n",
        "        df.loc[mask_10th, 'close'] = 1\n",
        "        count_10th = mask_10th.sum()\n",
        "        mask_5th_sunday = (df['building_number'] == building_num) & \\\n",
        "                          (df['day_of_week'] == 6) & \\\n",
        "                          (df['calendar_week_of_month'] == 5)\n",
        "        df.loc[mask_5th_sunday, 'close'] = 1\n",
        "        count_5th_sunday = mask_5th_sunday.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: ë§¤ë‹¬ 10ì¼ íœ´ì¼ {count_10th}ê°œ, 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ {count_5th_sunday}ê°œ ì ìš©\")\n",
        "\n",
        "    # 3. í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ë¬´ (32ë²ˆ ê±´ë¬¼) - closeì— ì ìš©\n",
        "    for building_num in odd_week_monday_buildings:\n",
        "        mask = (df['building_number'] == building_num) & \\\n",
        "               (df['day_of_week'] == 0) & \\\n",
        "               (df['calendar_week_of_month'] % 2 == 1)\n",
        "        df.loc[mask, 'close'] = 1\n",
        "        count = mask.sum()\n",
        "        print(f\"   ê±´ë¬¼ {building_num}: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ì¼ {count}ê°œ ì ìš©\")\n",
        "\n",
        "    # ì„ì‹œ ì»¬ëŸ¼ ì œê±°\n",
        "    df.drop(['calendar_week_of_month'], axis=1, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_inferred_holidays(df, building_info):\n",
        "    # ê·œì¹™ì´ ì—†ëŠ” ë°±í™”ì  ê±´ë¬¼ ë²ˆí˜¸ ì¶”ì¶œ\n",
        "    department_store_buildings = [19,34,45,54,73,74,79,88,95]\n",
        "\n",
        "    # ë°±í™”ì  ê±´ë¬¼ì— ëŒ€í•´ ì¶”ì • íœ´ì¼ë§Œ ì ìš©\n",
        "    for building_num in department_store_buildings:\n",
        "        building_data = df[df['building_number'] == building_num].copy()\n",
        "\n",
        "        if len(building_data) > 0:\n",
        "            # ì¼ë³„ ì „ë ¥ì†Œë¹„ëŸ‰ ê³„ì‚°\n",
        "            building_data['date'] = building_data['date_time'].dt.date\n",
        "            daily_consumption = building_data.groupby('date')['power_consumption'].sum()\n",
        "\n",
        "            # ì „ì²´ í‰ê·  ê¸°ì¤€ ì„ê³„ì¹˜ (0.7ë°°)\n",
        "            threshold = daily_consumption.mean() * 0.7\n",
        "            inferred_holiday_dates = daily_consumption[daily_consumption < threshold].index\n",
        "\n",
        "            # ì¶”ì • íœ´ì¼ì„ í•´ë‹¹ ê±´ë¬¼ì˜ holiday ì»¬ëŸ¼ì— ì ìš©\n",
        "            for holiday_date in inferred_holiday_dates:\n",
        "                mask = (df['building_number'] == building_num) & (df['date_time'].dt.date == holiday_date)\n",
        "                df.loc[mask, 'close'] = 1\n",
        "\n",
        "    return df\n",
        "\n",
        "# íœ´ì¼ í”¼ì²˜ ìƒì„±\n",
        "train = create_inferred_holidays(train, building_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì¤‘... (ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨)\n",
            "   ê±´ë¬¼ 18: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ì¼ 288ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 27: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 120ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 40: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 120ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 59: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 120ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 63: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 120ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 29: ë§¤ë‹¬ 10ì¼ íœ´ì¼ 72ê°œ, 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ 48ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 32: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ì¼ 144ê°œ ì ìš©\n",
            "ğŸ”§ íŠ¹ì • ê±´ë¬¼ íœ´ì¼ ê·œì¹™ ì ìš© ì¤‘... (ë‹¬ë ¥ ê¸°ì¤€ ì£¼ì°¨)\n",
            "   ê±´ë¬¼ 18: ë§¤ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 27: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 40: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 59: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 63: í™€ìˆ˜ ì£¼ ì¼ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 29: ë§¤ë‹¬ 10ì¼ íœ´ì¼ 0ê°œ, 5ë²ˆì§¸ ì£¼ ì¼ìš”ì¼ 24ê°œ ì ìš©\n",
            "   ê±´ë¬¼ 32: í™€ìˆ˜ ì£¼ ì›”ìš”ì¼ íœ´ì¼ 24ê°œ ì ìš©\n"
          ]
        }
      ],
      "source": [
        "train = apply_specific_building_holidays(train)\n",
        "test = apply_specific_building_holidays(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì„ ì£¼ë§/ê³µíœ´ì¼ì— close ì²˜ë¦¬ ì¤‘...\n",
            "ğŸ“‹ ëŒ€ìƒ ê±´ë¬¼ ìˆ˜: 41ê°œ\n",
            "   íŠ¹ì • ê±´ë¬¼: [6, 16, 20, 51, 86, 47, 69, 38, 50, 66, 68, 72, 80]\n",
            "   í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ: [3, 5, 8, 12, 13, 14, 15, 17, 21, 22, 23, 24, 37, 39, 42, 44, 46, 48, 49, 53, 55, 60, 62, 75, 83, 87, 90, 94]\n",
            "   ê±´ë¬¼ 6 (Commercial): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 16 (Commercial): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 20 (Commercial): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 51 (Commercial): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 86 (Commercial): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 47 (Other Buildings): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 69 (Other Buildings): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 38 (Public): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 50 (Public): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 66 (Public): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 68 (Public): 647ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 72 (Public): 646ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 80 (Public): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 3 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 5 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 8 (University): 645ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 12 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 13 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 14 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 15 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 17 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 21 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 22 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 23 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 24 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 37 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 39 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 42 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 44 (Hospital): 647ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 46 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 48 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 49 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 53 (Research Institute): 646ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 55 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 60 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 62 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 75 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 83 (Research Institute): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 87 (University): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 90 (Hospital): 648ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 94 (Research Institute): 646ê°œ close ì²˜ë¦¬\n",
            "\\nï¿½ï¿½ ì „ì²´ close ì²˜ë¦¬ ê²°ê³¼:\n",
            "   ì´ close ê±´ìˆ˜: 27973ê°œ\n",
            "ğŸ”§ íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì„ ì£¼ë§/ê³µíœ´ì¼ì— close ì²˜ë¦¬ ì¤‘...\n",
            "ğŸ“‹ ëŒ€ìƒ ê±´ë¬¼ ìˆ˜: 41ê°œ\n",
            "   íŠ¹ì • ê±´ë¬¼: [6, 16, 20, 51, 86, 47, 69, 38, 50, 66, 68, 72, 80]\n",
            "   í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ: [3, 5, 8, 12, 13, 14, 15, 17, 21, 22, 23, 24, 37, 39, 42, 44, 46, 48, 49, 53, 55, 60, 62, 75, 83, 87, 90, 94]\n",
            "   ê±´ë¬¼ 6 (Commercial): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 16 (Commercial): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 20 (Commercial): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 51 (Commercial): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 86 (Commercial): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 47 (Other Buildings): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 69 (Other Buildings): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 38 (Public): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 50 (Public): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 66 (Public): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 68 (Public): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 72 (Public): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 80 (Public): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 3 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 5 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 8 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 12 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 13 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 14 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 15 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 17 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 21 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 22 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 23 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 24 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 37 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 39 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 42 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 44 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 46 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 48 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 49 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 53 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 55 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 60 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 62 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 75 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 83 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 87 (University): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 90 (Hospital): 48ê°œ close ì²˜ë¦¬\n",
            "   ê±´ë¬¼ 94 (Research Institute): 48ê°œ close ì²˜ë¦¬\n",
            "\\nï¿½ï¿½ ì „ì²´ close ì²˜ë¦¬ ê²°ê³¼:\n",
            "   ì´ close ê±´ìˆ˜: 2136ê°œ\n",
            "âœ… íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì˜ ì£¼ë§/ê³µíœ´ì¼ close ì²˜ë¦¬ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì„ ì£¼ë§/ê³µíœ´ì¼ì— close ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
        "def apply_weekend_holiday_close_for_specific_buildings(df, building_info):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì„ ì£¼ë§ê³¼ ê³µíœ´ì¼ì— close ì²˜ë¦¬\n",
        "    \n",
        "    Args:\n",
        "        df: train ë˜ëŠ” test ë°ì´í„°í”„ë ˆì„\n",
        "        building_info: ê±´ë¬¼ ì •ë³´ ë°ì´í„°í”„ë ˆì„\n",
        "    \n",
        "    Returns:\n",
        "        df: close í”¼ì²˜ê°€ ì—…ë°ì´íŠ¸ëœ ë°ì´í„°í”„ë ˆì„\n",
        "    \"\"\"\n",
        "    print(\"ğŸ”§ íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì„ ì£¼ë§/ê³µíœ´ì¼ì— close ì²˜ë¦¬ ì¤‘...\")\n",
        "    \n",
        "    # íŠ¹ì • ê±´ë¬¼ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸\n",
        "    specific_buildings = [6, 16, 20, 51, 86, 47, 69, 38, 50, 66, 68, 72, 80]\n",
        "    \n",
        "    # í•™êµ, ë³‘ì›, ì—°êµ¬ì†Œ ê±´ë¬¼ ë²ˆí˜¸ ì¶”ì¶œ\n",
        "    school_hospital_research_buildings = building_info[\n",
        "        building_info['building_type'].isin(['University', 'Hospital', 'Research Institute'])\n",
        "    ]['building_number'].tolist()\n",
        "    \n",
        "    # ëª¨ë“  ëŒ€ìƒ ê±´ë¬¼ ë²ˆí˜¸\n",
        "    target_buildings = specific_buildings + school_hospital_research_buildings\n",
        "    \n",
        "    print(f\"ğŸ“‹ ëŒ€ìƒ ê±´ë¬¼ ìˆ˜: {len(target_buildings)}ê°œ\")\n",
        "    print(f\"   íŠ¹ì • ê±´ë¬¼: {specific_buildings}\")\n",
        "    print(f\"   í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ: {school_hospital_research_buildings}\")\n",
        "    \n",
        "    # ì£¼ë§ ë˜ëŠ” ê³µíœ´ì¼ì¸ ê²½ìš° close = 1ë¡œ ì„¤ì •\n",
        "    for building_num in target_buildings:\n",
        "        # í•´ë‹¹ ê±´ë¬¼ì˜ ë°ì´í„°ì—ì„œ ì£¼ë§ ë˜ëŠ” ê³µíœ´ì¼ì¸ ê²½ìš°\n",
        "        mask = (df['building_number'] == building_num) & \\\n",
        "               ((df['weekend'] == 1) | (df['holiday'] == 1))\n",
        "        \n",
        "        # close í”¼ì²˜ë¥¼ 1ë¡œ ì„¤ì •\n",
        "        df.loc[mask, 'close'] = 1\n",
        "        \n",
        "        # ì ìš©ëœ ê±´ìˆ˜ í™•ì¸\n",
        "        applied_count = mask.sum()\n",
        "        if applied_count > 0:\n",
        "            building_type = building_info[building_info['building_number'] == building_num]['building_type'].iloc[0]\n",
        "            print(f\"   ê±´ë¬¼ {building_num} ({building_type}): {applied_count}ê°œ close ì²˜ë¦¬\")\n",
        "    \n",
        "    # ì „ì²´ ì ìš© ê²°ê³¼ í™•ì¸\n",
        "    total_close_count = df[df['close'] == 1].shape[0]\n",
        "    print(f\"\\\\nï¿½ï¿½ ì „ì²´ close ì²˜ë¦¬ ê²°ê³¼:\")\n",
        "    print(f\"   ì´ close ê±´ìˆ˜: {total_close_count}ê°œ\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# í•¨ìˆ˜ ì‹¤í–‰\n",
        "train = apply_weekend_holiday_close_for_specific_buildings(train, building_info)\n",
        "test = apply_weekend_holiday_close_for_specific_buildings(test, building_info)\n",
        "\n",
        "print(\"âœ… íŠ¹ì • ê±´ë¬¼ë“¤ê³¼ í•™êµ/ë³‘ì›/ì—°êµ¬ì†Œ ê±´ë¬¼ë“¤ì˜ ì£¼ë§/ê³µíœ´ì¼ close ì²˜ë¦¬ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# íœ´ì¼ ì ìš© í™•ì¸ìš© \n",
        "def check_holiday(n,option='holiday'):\n",
        "    if option == 'holiday':\n",
        "        holiday_dates_n = train.loc[(train['building_number'] == n) & (train['holiday'] == 1), 'date_time'].dt.date\n",
        "        unique_holiday_dates_n = holiday_dates_n.drop_duplicates().tolist()\n",
        "        print(\"train ë°ì´í„° íœ´ì¼ í™•ì¸\")\n",
        "        print(f\"ê±´ë¬¼ {n}ë²ˆ  íœ´ì¼: {len(unique_holiday_dates_n)}ì¼\")\n",
        "        print(f\"ë‚ ì§œ: {unique_holiday_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "        holiday_dates_n = test.loc[(test['building_number'] == n) & (test['holiday'] == 1), 'date_time'].dt.date\n",
        "        unique_holiday_dates_n = holiday_dates_n.drop_duplicates().tolist()\n",
        "        print(\"test ë°ì´í„° íœ´ì¼ í™•ì¸\")\n",
        "        print(f\"ê±´ë¬¼ {n}ë²ˆ  íœ´ì¼: {len(unique_holiday_dates_n)}ì¼\")\n",
        "        print(f\"ë‚ ì§œ: {unique_holiday_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥        \n",
        "    if option == 'weekend':\n",
        "        weekend_dates_n = train.loc[(train['building_number'] == n) & (train['weekend'] == 1), 'date_time'].dt.date\n",
        "        unique_weekend_dates_n = weekend_dates_n.drop_duplicates().tolist()\n",
        "        print(\"train ë°ì´í„° ì£¼ë§ í™•ì¸\")\n",
        "        print(f\"ê±´ë¬¼ {n}ë²ˆ  ì£¼ë§: {len(unique_weekend_dates_n)}ì¼\")\n",
        "        print(f\"ë‚ ì§œ: {unique_weekend_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "        weekend_dates_n = test.loc[(test['building_number'] == n) & (test['weekend'] == 1), 'date_time'].dt.date\n",
        "        unique_weekend_dates_n = weekend_dates_n.drop_duplicates().tolist()\n",
        "        print(\"test ë°ì´í„° ì£¼ë§ í™•ì¸\")\n",
        "        print(f\"ê±´ë¬¼ {n}ë²ˆ  ì£¼ë§: {len(unique_weekend_dates_n)}ì¼\")\n",
        "        print(f\"ë‚ ì§œ: {unique_weekend_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "    if option == 'close':\n",
        "        close_dates_n = train.loc[(train['building_number'] == n) & (train['close'] == 1), 'date_time'].dt.date\n",
        "        unique_close_dates_n = close_dates_n.drop_duplicates().tolist()\n",
        "        print(\"train ë°ì´í„° ë‹«ì€ë‚  í™•ì¸\")\n",
        "        print(f\"ê±´ë¬¼ {n}ë²ˆ  ë‹«ì€ë‚ : {len(unique_close_dates_n)}ì¼\")\n",
        "        print(f\"ë‚ ì§œ: {unique_close_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "        close_dates_n = test.loc[(test['building_number'] == n) & (test['close'] == 1), 'date_time'].dt.date\n",
        "        unique_close_dates_n = close_dates_n.drop_duplicates().tolist()\n",
        "        print(\"test ë°ì´í„° ë‹«ì€ë‚  í™•ì¸\")\n",
        "        print(f\"ê±´ë¬¼ {n}ë²ˆ  ë‹«ì€ë‚ : {len(unique_close_dates_n)}ì¼\")\n",
        "        print(f\"ë‚ ì§œ: {unique_close_dates_n[:10]}...\")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train ë°ì´í„° ë‹«ì€ë‚  í™•ì¸\n",
            "ê±´ë¬¼ 19ë²ˆ  ë‹«ì€ë‚ : 3ì¼\n",
            "ë‚ ì§œ: [datetime.date(2024, 6, 10), datetime.date(2024, 7, 8), datetime.date(2024, 8, 19)]...\n",
            "test ë°ì´í„° ë‹«ì€ë‚  í™•ì¸\n",
            "ê±´ë¬¼ 19ë²ˆ  ë‹«ì€ë‚ : 0ì¼\n",
            "ë‚ ì§œ: []...\n"
          ]
        }
      ],
      "source": [
        "check_holiday(19,'close')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ê¸°ìƒ ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ìƒ ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„±\n",
        "def CDH(xs):\n",
        "    \"\"\"Cooling Degree Hours ê³„ì‚°\"\"\"\n",
        "    cumsum = np.cumsum(xs - 26)\n",
        "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
        "\n",
        "def calculate_and_add_cdh(dataframe):\n",
        "    \"\"\"ê±´ë¬¼ë³„ CDH ê³„ì‚° ë° ì¶”ê°€\"\"\"\n",
        "    cdhs = []\n",
        "    for i in range(1, 101):\n",
        "        temp = dataframe[dataframe['building_number'] == i]['temperature'].values\n",
        "        cdh = CDH(temp)\n",
        "        cdhs.append(cdh)\n",
        "    return np.concatenate(cdhs)\n",
        "\n",
        "# CDH, THI, WCT í”¼ì²˜ ìƒì„±\n",
        "train['CDH'] = calculate_and_add_cdh(train)\n",
        "test['CDH'] = calculate_and_add_cdh(test)\n",
        "\n",
        "# THI (Temperature Humidity Index)\n",
        "train['THI'] = 9/5 * train['temperature'] - 0.55 * (1 - train['humidity']/100) * (9/5 * train['temperature'] - 26) + 32\n",
        "test['THI'] = 9/5 * test['temperature'] - 0.55 * (1 - test['humidity']/100) * (9/5 * test['temperature'] - 26) + 32\n",
        "\n",
        "# WCT (Wind Chill Temperature)\n",
        "train['WCT'] = 13.12 + 0.6125 * train['temperature'] - 11.37 * (train['windspeed']**0.16) + 0.3965 * (train['windspeed']**0.16) * train['temperature']\n",
        "test['WCT'] = 13.12 + 0.6125 * test['temperature'] - 11.37 * (test['windspeed']**0.16) + 0.3965 * (test['windspeed']**0.16) * test['temperature']\n",
        "\n",
        "print(\"âœ… ê¸°ìƒ ê´€ë ¨ íŒŒìƒ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì¤‘...\n",
            "âœ… ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\n",
            "ìµœì¢… train ë°ì´í„° shape: (203864, 43)\n",
            "ìµœì¢… test ë°ì´í„° shape: (16800, 40)\n"
          ]
        }
      ],
      "source": [
        "# ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± (Target-like Features)\n",
        "print(\"ğŸ“Š ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ê±´ë¬¼ë³„ ì‹œê°„ëŒ€/ìš”ì¼ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
        "power_mean = pd.pivot_table(train, values='power_consumption', \n",
        "                           index=['building_number', 'hour', 'day_of_week'], \n",
        "                           aggfunc=np.mean).reset_index()\n",
        "power_mean.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_mean']\n",
        "\n",
        "power_std = pd.pivot_table(train, values='power_consumption', \n",
        "                          index=['building_number', 'hour', 'day_of_week'], \n",
        "                          aggfunc=np.std).reset_index()\n",
        "power_std.columns = ['building_number', 'hour', 'day_of_week', 'day_hour_std']\n",
        "\n",
        "# ê±´ë¬¼ë³„ ì‹œê°„ëŒ€ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
        "power_hour_mean = pd.pivot_table(train, values='power_consumption', \n",
        "                                index=['building_number', 'hour'], \n",
        "                                aggfunc=np.mean).reset_index()\n",
        "power_hour_mean.columns = ['building_number', 'hour', 'hour_mean']\n",
        "\n",
        "power_hour_std = pd.pivot_table(train, values='power_consumption', \n",
        "                               index=['building_number', 'hour'], \n",
        "                               aggfunc=np.std).reset_index()\n",
        "power_hour_std.columns = ['building_number', 'hour', 'hour_std']\n",
        "\n",
        "# í†µê³„ í”¼ì²˜ ë³‘í•©\n",
        "train = train.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "train = train.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "train = train.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
        "train = train.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
        "\n",
        "test = test.merge(power_mean, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "test = test.merge(power_std, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
        "test = test.merge(power_hour_mean, on=['building_number', 'hour'], how='left')\n",
        "test = test.merge(power_hour_std, on=['building_number', 'hour'], how='left')\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "print(\"âœ… ì „ë ¥ ì†Œë¹„ëŸ‰ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"ìµœì¢… train ë°ì´í„° shape: {train.shape}\")\n",
        "print(f\"ìµœì¢… test ë°ì´í„° shape: {test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ TimesNet ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„ ì‹œì‘!\n",
            "============================================================\n",
            "âœ… ìˆ˜ì •ëœ TimesNet í›ˆë ¨ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (ì •í™•í•œ SMAPE ê³„ì‚°)\n",
            "ğŸ“Š TimesNetìš© í”¼ì²˜ ì„¤ì •...\n",
            "ì‚¬ìš©í•  í”¼ì²˜ (22ê°œ): ['temperature', 'windspeed', 'humidity', 'rainfall', 'sunshine', 'solar_radiation', 'sin_hour', 'cos_hour', 'sin_dayofweek', 'cos_dayofweek', 'sin_month', 'cos_month', 'total_area', 'cooling_area', 'CDH', 'THI', 'WCT', 'day_hour_mean', 'hour_mean', 'close', 'weekend', 'holiday']\n",
            "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
            "âœ… TimesNet ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "## ğŸŒŸ TimesNet ëª¨ë¸ êµ¬í˜„ (Time-Series-Library ê¸°ë°˜)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "print(\"ğŸš€ TimesNet ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„ ì‹œì‘!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. TimesNet í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, seq_len, pred_len, top_k, d_model, d_ff, num_kernels=6):\n",
        "        super(TimesBlock, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.k = top_k\n",
        "        \n",
        "        # FFTë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„°\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1),\n",
        "        )\n",
        "        \n",
        "        # 2D ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë“¤\n",
        "        self.conv2d_layers = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=d_model, out_channels=d_model, \n",
        "                     kernel_size=(3, 3), padding=(1, 1))\n",
        "            for _ in range(num_kernels)\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        B, T, N = x.shape\n",
        "        \n",
        "        # 1. FFTë¥¼ í†µí•œ ì£¼íŒŒìˆ˜ ë¶„ì„\n",
        "        x_fft = torch.fft.rfft(x.permute(0, 2, 1), dim=-1)  # (B, N, T//2+1)\n",
        "        \n",
        "        # 2. ìƒìœ„ kê°œ ì£¼íŒŒìˆ˜ ì„ íƒ\n",
        "        amplitude = torch.abs(x_fft)\n",
        "        _, top_indices = torch.topk(amplitude.mean(dim=1), self.k, dim=-1)\n",
        "        \n",
        "        # 3. 2D ë³€í™˜ ë° ì²˜ë¦¬\n",
        "        period_list = []\n",
        "        for i in range(self.k):\n",
        "            period = T // (top_indices[:, i] + 1)  # ì£¼ê¸° ê³„ì‚°\n",
        "            period = torch.clamp(period, min=1, max=T)\n",
        "            period_list.append(period)\n",
        "        \n",
        "        # 4. ê° ì£¼ê¸°ë³„ë¡œ 2D ë³€í™˜í•˜ì—¬ ì²˜ë¦¬\n",
        "        res = []\n",
        "        for i in range(self.k):\n",
        "            period = period_list[i]\n",
        "            \n",
        "            # 2D ë³€í™˜: (batch_size, seq_len, d_model) -> (batch_size, d_model, period, seq_len//period)\n",
        "            if T % period.max() != 0:\n",
        "                length = (T // period.max() + 1) * period.max()\n",
        "                padding = torch.zeros([B, (length - T), N]).to(x.device)\n",
        "                out = torch.cat([x, padding], dim=1)\n",
        "            else:\n",
        "                length = T\n",
        "                out = x\n",
        "            \n",
        "            # Reshape to 2D\n",
        "            out = out.reshape(B, length // period.max(), period.max(), N)\n",
        "            out = out.permute(0, 3, 1, 2).contiguous()  # (B, N, length//period, period)\n",
        "            \n",
        "            # 2D ì»¨ë³¼ë£¨ì…˜ ì ìš©\n",
        "            out = self.conv2d_layers[i](out)\n",
        "            \n",
        "            # ë‹¤ì‹œ 1Dë¡œ ë³€í™˜\n",
        "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
        "            out = out[:, :T, :]\n",
        "            res.append(out)\n",
        "        \n",
        "        # 5. ê²°ê³¼ í•©ì„±\n",
        "        res = torch.stack(res, dim=-1)\n",
        "        res = torch.mean(res, dim=-1)\n",
        "        \n",
        "        # 6. Residual connectionê³¼ ì •ê·œí™”\n",
        "        res = res + x\n",
        "        res = self.norm(res)\n",
        "        \n",
        "        return res\n",
        "\n",
        "class TimesNet(nn.Module):\n",
        "    def __init__(self, \n",
        "                 seq_len=168,\n",
        "                 pred_len=1, \n",
        "                 enc_in=22,\n",
        "                 d_model=64,\n",
        "                 d_ff=128,\n",
        "                 e_layers=3,\n",
        "                 top_k=5,\n",
        "                 num_kernels=6,\n",
        "                 dropout=0.1):\n",
        "        super(TimesNet, self).__init__()\n",
        "        \n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.model_type = 'TimesNet'\n",
        "        \n",
        "        # ì…ë ¥ ì„ë² ë”©\n",
        "        self.enc_embedding = nn.Linear(enc_in, d_model)\n",
        "        \n",
        "        # TimesBlock ë ˆì´ì–´ë“¤\n",
        "        self.encoder = nn.ModuleList([\n",
        "            TimesBlock(seq_len, pred_len, top_k, d_model, d_ff, num_kernels)\n",
        "            for _ in range(e_layers)\n",
        "        ])\n",
        "        \n",
        "        # ì •ê·œí™” ë° ë“œë¡­ì•„ì›ƒ\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # ì˜ˆì¸¡ í—¤ë“œ\n",
        "        self.projection = nn.Linear(d_model, pred_len)\n",
        "        \n",
        "        print(f\"   TimesNet ì´ˆê¸°í™”: seq_len={seq_len}, pred_len={pred_len}, enc_in={enc_in}\")\n",
        "        print(f\"   d_model={d_model}, layers={e_layers}, top_k={top_k}\")\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, enc_in)\n",
        "        \n",
        "        # ì…ë ¥ ì„ë² ë”©\n",
        "        enc_out = self.enc_embedding(x)  # (batch_size, seq_len, d_model)\n",
        "        enc_out = self.dropout(enc_out)\n",
        "        \n",
        "        # TimesBlock ë ˆì´ì–´ë“¤ í†µê³¼\n",
        "        for layer in self.encoder:\n",
        "            enc_out = layer(enc_out)\n",
        "        \n",
        "        # ì •ê·œí™”\n",
        "        enc_out = self.layer_norm(enc_out)\n",
        "        \n",
        "        # ì˜ˆì¸¡: ë§ˆì§€ë§‰ ì‹œì ì˜ íŠ¹ì§•ì„ ì‚¬ìš©\n",
        "        output = self.projection(enc_out[:, -1, :])  # (batch_size, pred_len)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# 2. TimesNetìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
        "class TimesNetDataset(Dataset):\n",
        "    def __init__(self, data, target, seq_len=168, pred_len=1):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        \n",
        "        print(f\"   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape={data.shape}, seq_len={seq_len}\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return max(0, len(self.data) - self.seq_len - self.pred_len + 1)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.seq_len]  # (seq_len, num_features)\n",
        "        y = self.target[idx + self.seq_len:idx + self.seq_len + self.pred_len]  # (pred_len,)\n",
        "        return torch.FloatTensor(x), torch.FloatTensor(y)\n",
        "\n",
        "# 3. TimesNet í›ˆë ¨ í•¨ìˆ˜\n",
        "# ğŸ”§ ìˆ˜ì •ëœ train_timesnet í•¨ìˆ˜ (ì •í™•í•œ SMAPE ê³„ì‚°)\n",
        "\n",
        "def train_timesnet_fixed(model, train_loader, val_loader, scaler_y, epochs=1000, learning_rate=0.01):\n",
        "    \"\"\"ìˆ˜ì •ëœ TimesNet ëª¨ë¸ í›ˆë ¨ (ì •í™•í•œ SMAPE ê³„ì‚°)\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # í›ˆë ¨\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_count = 0\n",
        "        \n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            try:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y.squeeze())\n",
        "                loss.backward()\n",
        "                \n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                \n",
        "                train_loss += loss.item()\n",
        "                train_count += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                if batch_idx < 3:\n",
        "                    print(f\"   í›ˆë ¨ ë°°ì¹˜ {batch_idx} ì˜¤ë¥˜: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if train_count == 0:\n",
        "            break\n",
        "            \n",
        "        train_loss /= train_count\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # ê²€ì¦\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_count = 0\n",
        "        val_predictions_raw = []  # ì •ê·œí™”ëœ ê°’ (loss ê³„ì‚°ìš©)\n",
        "        val_targets_raw = []\n",
        "        val_predictions_denorm = []  # ì—­ì •ê·œí™”ëœ ê°’ (SMAPE ê³„ì‚°ìš©)\n",
        "        val_targets_denorm = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (batch_x, batch_y) in enumerate(val_loader):\n",
        "                try:\n",
        "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                    \n",
        "                    outputs = model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y.squeeze())\n",
        "                    val_loss += loss.item()\n",
        "                    val_count += 1\n",
        "                    \n",
        "                    # ì •ê·œí™”ëœ ê°’ ì €ì¥ (loss ê³„ì‚°ìš©)\n",
        "                    val_predictions_raw.extend(outputs.cpu().numpy())\n",
        "                    val_targets_raw.extend(batch_y.cpu().numpy().flatten())\n",
        "                    \n",
        "                    # ğŸ¯ ì—­ì •ê·œí™”í•´ì„œ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜ (SMAPE ê³„ì‚°ìš©)\n",
        "                    pred_denorm = scaler_y.inverse_transform(outputs.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                    true_denorm = scaler_y.inverse_transform(batch_y.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                    \n",
        "                    val_predictions_denorm.extend(pred_denorm)\n",
        "                    val_targets_denorm.extend(true_denorm)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    if batch_idx < 3:\n",
        "                        print(f\"   ê²€ì¦ ë°°ì¹˜ {batch_idx} ì˜¤ë¥˜: {e}\")\n",
        "                    continue\n",
        "        \n",
        "        if val_count == 0:\n",
        "            break\n",
        "            \n",
        "        val_loss /= val_count\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Early Stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= patience:\n",
        "            print(f\"   ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ {epoch+1}\")\n",
        "            break\n",
        "        \n",
        "        # ğŸ¯ ì •í™•í•œ SMAPEë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            if val_predictions_denorm and val_targets_denorm:\n",
        "                # ì‹¤ì œ ê°’ìœ¼ë¡œ SMAPE ê³„ì‚°\n",
        "                val_smape = smape(val_targets_denorm, val_predictions_denorm)\n",
        "                print(f\"   ğŸ“Š ì—í¬í¬ {epoch+1:3d}: í›ˆë ¨ ì†ì‹¤ {train_loss:.6f}, ê²€ì¦ ì†ì‹¤ {val_loss:.6f}, SMAPE {val_smape:.4f}\")\n",
        "            else:\n",
        "                print(f\"   ğŸ“Š ì—í¬í¬ {epoch+1:3d}: í›ˆë ¨ ì†ì‹¤ {train_loss:.6f}, ê²€ì¦ ì†ì‹¤ {val_loss:.6f}\")\n",
        "    \n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "print(\"âœ… ìˆ˜ì •ëœ TimesNet í›ˆë ¨ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ (ì •í™•í•œ SMAPE ê³„ì‚°)\")\n",
        "\n",
        "# 4. í”¼ì²˜ ì„¤ì •\n",
        "print(\"ğŸ“Š TimesNetìš© í”¼ì²˜ ì„¤ì •...\")\n",
        "feature_columns = [\n",
        "    'temperature', 'windspeed', 'humidity', 'rainfall', 'sunshine', 'solar_radiation',\n",
        "    'sin_hour', 'cos_hour', 'sin_dayofweek', 'cos_dayofweek', 'sin_month', 'cos_month',\n",
        "    'total_area', 'cooling_area', 'CDH', 'THI', 'WCT',\n",
        "    'day_hour_mean', 'hour_mean', 'close', 'weekend', 'holiday'\n",
        "]\n",
        "\n",
        "# ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ\n",
        "available_features = [f for f in feature_columns if f in train.columns]\n",
        "print(f\"ì‚¬ìš©í•  í”¼ì²˜ ({len(available_features)}ê°œ): {available_features}\")\n",
        "\n",
        "NUM_FEATURES = len(available_features)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "# 5. ë°ì´í„° ë¶„í• \n",
        "train_mask = train['date_time'] < pd.to_datetime('2024-08-17')\n",
        "val_mask = train['date_time'] >= pd.to_datetime('2024-08-17')\n",
        "\n",
        "print(\"âœ… TimesNet ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ—ï¸ ìˆ˜ì •ëœ TimesNet ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ í›ˆë ¨...\n",
            "============================================================\n",
            "\n",
            "ğŸ” ê±´ë¬¼ ìœ í˜•: Hotel\n",
            "   ğŸ“Š ë°ì´í„° shape: X_train=(18479, 22), y_train=(18479,)\n",
            "   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape=(18479, 22), seq_len=168\n",
            "   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape=(1920, 22), seq_len=168\n",
            "   TimesNet ì´ˆê¸°í™”: seq_len=168, pred_len=1, enc_in=22\n",
            "   d_model=64, layers=2, top_k=5\n",
            "   ğŸ“Š ì—í¬í¬  10: í›ˆë ¨ ì†ì‹¤ 1.005131, ê²€ì¦ ì†ì‹¤ 1.104926, SMAPE 68.1131\n",
            "   ğŸ“Š ì—í¬í¬  20: í›ˆë ¨ ì†ì‹¤ 1.004709, ê²€ì¦ ì†ì‹¤ 1.108591, SMAPE 67.6555\n",
            "   ğŸ“Š ì—í¬í¬  30: í›ˆë ¨ ì†ì‹¤ 1.003748, ê²€ì¦ ì†ì‹¤ 1.111856, SMAPE 67.5463\n",
            "   ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ 32\n",
            "   âœ… ìµœì¢… TimesNet SMAPE: 67.6401\n",
            "\n",
            "ğŸ” ê±´ë¬¼ ìœ í˜•: Commercial\n",
            "   ğŸ“Š ë°ì´í„° shape: X_train=(18467, 22), y_train=(18467,)\n",
            "   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape=(18467, 22), seq_len=168\n",
            "   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape=(1920, 22), seq_len=168\n",
            "   TimesNet ì´ˆê¸°í™”: seq_len=168, pred_len=1, enc_in=22\n",
            "   d_model=64, layers=2, top_k=5\n",
            "   ğŸ“Š ì—í¬í¬  10: í›ˆë ¨ ì†ì‹¤ 1.000476, ê²€ì¦ ì†ì‹¤ 1.071715, SMAPE 47.0140\n",
            "   ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: ì—í¬í¬ 18\n",
            "   âœ… ìµœì¢… TimesNet SMAPE: 46.9929\n",
            "\n",
            "ğŸ” ê±´ë¬¼ ìœ í˜•: Hospital\n",
            "   ğŸ“Š ë°ì´í„° shape: X_train=(16614, 22), y_train=(16614,)\n",
            "   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape=(16614, 22), seq_len=168\n",
            "   TimesNet ë°ì´í„°ì…‹ ì´ˆê¸°í™”: data shape=(1728, 22), seq_len=168\n",
            "   TimesNet ì´ˆê¸°í™”: seq_len=168, pred_len=1, enc_in=22\n",
            "   d_model=64, layers=2, top_k=5\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 64\u001b[0m\n\u001b[0;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m TimesNet(\n\u001b[0;32m     52\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39mseq_len,\n\u001b[0;32m     53\u001b[0m     pred_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m     61\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# ğŸ”§ ìˆ˜ì •ëœ ëª¨ë¸ í›ˆë ¨ (scaler_y ì „ë‹¬)\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_timesnet_fixed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[0;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# ìµœì¢… ì„±ëŠ¥ í‰ê°€\u001b[39;00m\n\u001b[0;32m     70\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
            "Cell \u001b[1;32mIn[31], line 195\u001b[0m, in \u001b[0;36mtrain_timesnet_fixed\u001b[1;34m(model, train_loader, val_loader, scaler_y, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    192\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    194\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m    197\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\minkyu\\AppData\\Local\\anaconda3\\envs\\sd2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\minkyu\\AppData\\Local\\anaconda3\\envs\\sd2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[31], line 140\u001b[0m, in \u001b[0;36mTimesNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# TimesBlock ë ˆì´ì–´ë“¤ í†µê³¼\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[1;32m--> 140\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# ì •ê·œí™”\u001b[39;00m\n\u001b[0;32m    143\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(enc_out)\n",
            "File \u001b[1;32mc:\\Users\\minkyu\\AppData\\Local\\anaconda3\\envs\\sd2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\minkyu\\AppData\\Local\\anaconda3\\envs\\sd2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[31], line 65\u001b[0m, in \u001b[0;36mTimesBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m period \u001b[38;5;241m=\u001b[39m period_list[i]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 2D ë³€í™˜: (batch_size, seq_len, d_model) -> (batch_size, d_model, period, seq_len//period)\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     length \u001b[38;5;241m=\u001b[39m (T \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m period\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m period\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m     67\u001b[0m     padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([B, (length \u001b[38;5;241m-\u001b[39m T), N])\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\minkyu\\AppData\\Local\\anaconda3\\envs\\sd2\\lib\\site-packages\\torch\\_tensor.py:39\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\minkyu\\AppData\\Local\\anaconda3\\envs\\sd2\\lib\\site-packages\\torch\\_tensor.py:1046\u001b[0m, in \u001b[0;36mTensor.__rmod__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__rmod__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremainder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## ğŸ—ï¸ ìˆ˜ì •ëœ TimesNet ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ í›ˆë ¨\n",
        "\n",
        "print(\"\\nğŸ—ï¸ ìˆ˜ì •ëœ TimesNet ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ í›ˆë ¨...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "building_types = train['building_type'].unique()\n",
        "timesnet_type_models_fixed = {}\n",
        "timesnet_type_scores_fixed = {}\n",
        "\n",
        "for building_type in building_types:\n",
        "    print(f\"\\nğŸ” ê±´ë¬¼ ìœ í˜•: {building_type}\")\n",
        "    \n",
        "    # í•´ë‹¹ ìœ í˜• ë°ì´í„° í•„í„°ë§\n",
        "    type_train_data = train[(train['building_type'] == building_type) & train_mask].sort_values('date_time')\n",
        "    type_val_data = train[(train['building_type'] == building_type) & val_mask].sort_values('date_time')\n",
        "    \n",
        "    if len(type_train_data) < 500 or len(type_val_data) < 100:\n",
        "        print(f\"   âŒ ë°ì´í„° ë¶€ì¡± (í›ˆë ¨: {len(type_train_data)}, ê²€ì¦: {len(type_val_data)}), ê±´ë„ˆëœ€\")\n",
        "        continue\n",
        "    \n",
        "    # í”¼ì²˜ì™€ íƒ€ê²Ÿ ì¤€ë¹„\n",
        "    X_train = type_train_data[available_features].fillna(0).values\n",
        "    y_train = type_train_data['power_consumption'].values\n",
        "    X_val = type_val_data[available_features].fillna(0).values\n",
        "    y_val = type_val_data['power_consumption'].values\n",
        "    \n",
        "    print(f\"   ğŸ“Š ë°ì´í„° shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "    \n",
        "    # ì •ê·œí™”\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    \n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    # TimesNet ë°ì´í„°ì…‹ ìƒì„±\n",
        "    seq_len = min(168, len(X_train_scaled) // 10)\n",
        "    train_dataset = TimesNetDataset(X_train_scaled, y_train_scaled, seq_len=seq_len, pred_len=1)\n",
        "    val_dataset = TimesNetDataset(X_val_scaled, y_val_scaled, seq_len=seq_len, pred_len=1)\n",
        "    \n",
        "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
        "        print(f\"   âŒ ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±, ê±´ë„ˆëœ€\")\n",
        "        continue\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
        "    \n",
        "    # TimesNet ëª¨ë¸ ì´ˆê¸°í™”\n",
        "    model = TimesNet(\n",
        "        seq_len=seq_len,\n",
        "        pred_len=1,\n",
        "        enc_in=NUM_FEATURES,\n",
        "        d_model=64,\n",
        "        d_ff=128,\n",
        "        e_layers=2,\n",
        "        top_k=5,\n",
        "        num_kernels=6,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "    \n",
        "    # ğŸ”§ ìˆ˜ì •ëœ ëª¨ë¸ í›ˆë ¨ (scaler_y ì „ë‹¬)\n",
        "    model, train_losses, val_losses = train_timesnet_fixed(\n",
        "        model, train_loader, val_loader, scaler_y, \n",
        "        epochs=100, learning_rate=0.001\n",
        "    )\n",
        "    \n",
        "    # ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            try:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_x)\n",
        "                \n",
        "                # ì—­ì •ê·œí™”\n",
        "                pred_denorm = scaler_y.inverse_transform(outputs.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                true_denorm = scaler_y.inverse_transform(batch_y.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                \n",
        "                val_predictions.extend(pred_denorm)\n",
        "                val_targets.extend(true_denorm)\n",
        "            except Exception as e:\n",
        "                print(f\"   í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if val_predictions and val_targets:\n",
        "        # ìµœì¢… SMAPE ê³„ì‚°\n",
        "        final_smape = smape(val_targets, val_predictions)\n",
        "        timesnet_type_scores_fixed[building_type] = final_smape\n",
        "        \n",
        "        # ëª¨ë¸ ì €ì¥\n",
        "        timesnet_type_models_fixed[building_type] = {\n",
        "            'model': model,\n",
        "            'scaler_X': scaler_X,\n",
        "            'scaler_y': scaler_y,\n",
        "            'seq_len': seq_len\n",
        "        }\n",
        "        \n",
        "        print(f\"   âœ… ìµœì¢… TimesNet SMAPE: {final_smape:.4f}\")\n",
        "    else:\n",
        "        print(f\"   âŒ í‰ê°€ ì‹¤íŒ¨\")\n",
        "\n",
        "print(f\"\\nâœ… ìˆ˜ì •ëœ TimesNet ê±´ë¬¼ íƒ€ì…ë³„ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ ({len(timesnet_type_models_fixed)}ê°œ)\")\n",
        "\n",
        "# ì„±ëŠ¥ ë¹„êµ\n",
        "if timesnet_type_scores_fixed:\n",
        "    print(\"\\nğŸ“Š ìˆ˜ì •ëœ TimesNet ì„±ëŠ¥ ê²°ê³¼:\")\n",
        "    print(\"-\" * 60)\n",
        "    for building_type in timesnet_type_scores_fixed.keys():\n",
        "        fixed_score = timesnet_type_scores_fixed[building_type]\n",
        "        print(f\"   {building_type:15s}: SMAPE {fixed_score:7.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ  ìˆ˜ì •ëœ TimesNet ê°œë³„ ê±´ë¬¼ë³„ ëª¨ë¸ í›ˆë ¨\n",
        "\n",
        "print(\"\\nğŸ  ìˆ˜ì •ëœ TimesNet ê°œë³„ ê±´ë¬¼ë³„ ëª¨ë¸ í›ˆë ¨...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ìƒìœ„ 5ê°œ ê±´ë¬¼ë§Œ í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì ˆì•½)\n",
        "building_numbers = sorted(train['building_number'].unique())[:5]\n",
        "timesnet_individual_models_fixed = {}\n",
        "timesnet_individual_scores_fixed = {}\n",
        "\n",
        "for building_num in building_numbers:\n",
        "    print(f\"\\nğŸ¢ ê±´ë¬¼ {building_num} ìˆ˜ì •ëœ TimesNet í›ˆë ¨ ì¤‘...\")\n",
        "    \n",
        "    # í•´ë‹¹ ê±´ë¬¼ ë°ì´í„° í•„í„°ë§\n",
        "    building_train_data = train[(train['building_number'] == building_num) & train_mask].sort_values('date_time')\n",
        "    building_val_data = train[(train['building_number'] == building_num) & val_mask].sort_values('date_time')\n",
        "    \n",
        "    if len(building_train_data) < 500 or len(building_val_data) < 50:\n",
        "        print(f\"   âŒ ë°ì´í„° ë¶€ì¡± (í›ˆë ¨: {len(building_train_data)}, ê²€ì¦: {len(building_val_data)}), ê±´ë„ˆëœ€\")\n",
        "        continue\n",
        "    \n",
        "    # í”¼ì²˜ì™€ íƒ€ê²Ÿ ì¤€ë¹„\n",
        "    X_train = building_train_data[available_features].fillna(0).values\n",
        "    y_train = building_train_data['power_consumption'].values\n",
        "    X_val = building_val_data[available_features].fillna(0).values\n",
        "    y_val = building_val_data['power_consumption'].values\n",
        "    \n",
        "    print(f\"   ğŸ“Š ë°ì´í„° shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "    \n",
        "    # ì •ê·œí™”\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    \n",
        "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "    X_val_scaled = scaler_X.transform(X_val)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "    y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    # TimesNet ë°ì´í„°ì…‹ ìƒì„±\n",
        "    seq_len = min(168, len(X_train_scaled) // 10)\n",
        "    train_dataset = TimesNetDataset(X_train_scaled, y_train_scaled, seq_len=seq_len, pred_len=1)\n",
        "    val_dataset = TimesNetDataset(X_val_scaled, y_val_scaled, seq_len=seq_len, pred_len=1)\n",
        "    \n",
        "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
        "        print(f\"   âŒ ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±, ê±´ë„ˆëœ€\")\n",
        "        continue\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, drop_last=True)\n",
        "    \n",
        "    # ê°œë³„ ê±´ë¬¼ìš© ë” ì‘ì€ TimesNet ëª¨ë¸\n",
        "    model = TimesNet(\n",
        "        seq_len=seq_len,\n",
        "        pred_len=1,\n",
        "        enc_in=NUM_FEATURES,\n",
        "        d_model=32,  # ë” ì‘ê²Œ\n",
        "        d_ff=64,\n",
        "        e_layers=2,\n",
        "        top_k=3,     # ë” ì‘ê²Œ\n",
        "        num_kernels=4,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "    \n",
        "    # ğŸ”§ ìˆ˜ì •ëœ ëª¨ë¸ í›ˆë ¨\n",
        "    model, _, _ = train_timesnet_fixed(\n",
        "        model, train_loader, val_loader, scaler_y, \n",
        "        epochs=50, learning_rate=0.001\n",
        "    )\n",
        "    \n",
        "    # ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
        "    model.eval()\n",
        "    val_predictions = []\n",
        "    val_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            try:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_x)\n",
        "                \n",
        "                # ì—­ì •ê·œí™”\n",
        "                pred_denorm = scaler_y.inverse_transform(outputs.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                true_denorm = scaler_y.inverse_transform(batch_y.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                \n",
        "                val_predictions.extend(pred_denorm)\n",
        "                val_targets.extend(true_denorm)\n",
        "            except Exception as e:\n",
        "                print(f\"   í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if val_predictions and val_targets:\n",
        "        # ìµœì¢… SMAPE ê³„ì‚°\n",
        "        final_smape = smape(val_targets, val_predictions)\n",
        "        timesnet_individual_scores_fixed[building_num] = final_smape\n",
        "        \n",
        "        # ëª¨ë¸ ì €ì¥\n",
        "        timesnet_individual_models_fixed[building_num] = {\n",
        "            'model': model,\n",
        "            'scaler_X': scaler_X,\n",
        "            'scaler_y': scaler_y,\n",
        "            'seq_len': seq_len\n",
        "        }\n",
        "        \n",
        "        print(f\"   âœ… ìµœì¢… TimesNet SMAPE: {final_smape:.4f}\")\n",
        "    else:\n",
        "        print(f\"   âŒ í‰ê°€ ì‹¤íŒ¨\")\n",
        "\n",
        "print(f\"\\nâœ… ìˆ˜ì •ëœ TimesNet ê°œë³„ ê±´ë¬¼ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ ({len(timesnet_individual_models_fixed)}ê°œ)\")\n",
        "\n",
        "# ì„±ëŠ¥ ë¹„êµ\n",
        "if timesnet_individual_scores_fixed:\n",
        "    print(\"\\nğŸ“Š ìˆ˜ì •ëœ TimesNet ê°œë³„ ê±´ë¬¼ ì„±ëŠ¥:\")\n",
        "    print(\"-\" * 60)\n",
        "    for building_num in timesnet_individual_scores_fixed.keys():\n",
        "        fixed_score = timesnet_individual_scores_fixed[building_num]\n",
        "        print(f\"   ê±´ë¬¼ {building_num:2d}: SMAPE {fixed_score:7.4f}\")\n",
        "\n",
        "print(\"\\nğŸ‰ ìˆ˜ì •ëœ TimesNet í›ˆë ¨ ì½”ë“œ ì ìš© ì™„ë£Œ!\")\n",
        "print(\"ì´ì œ í›ˆë ¨ ì¤‘ SMAPEì™€ ìµœì¢… SMAPEê°€ ì¼ì¹˜í•©ë‹ˆë‹¤! âœ…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ”® TimesNet í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥\n",
        "\n",
        "print(\"\\nğŸ”® TimesNet í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def predict_with_timesnet(model_dict, test_data, feature_columns):\n",
        "    \"\"\"TimesNet ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\"\"\"\n",
        "    model = model_dict['model']\n",
        "    scaler_X = model_dict['scaler_X']\n",
        "    scaler_y = model_dict['scaler_y']\n",
        "    seq_len = model_dict['seq_len']\n",
        "    \n",
        "    # í”¼ì²˜ ì¤€ë¹„\n",
        "    X_test = test_data[feature_columns].fillna(0).values\n",
        "    X_test_scaled = scaler_X.transform(X_test)\n",
        "    predictions = []\n",
        "    \n",
        "    if len(X_test_scaled) >= seq_len:\n",
        "        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì˜ˆì¸¡\n",
        "        for i in range(len(X_test_scaled)):\n",
        "            try:\n",
        "                if i < seq_len:\n",
        "                    seq_data = X_test_scaled[:seq_len]\n",
        "                else:\n",
        "                    seq_data = X_test_scaled[i-seq_len:i]\n",
        "                \n",
        "                seq_tensor = torch.FloatTensor(seq_data).unsqueeze(0).to(device)\n",
        "                \n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    pred_scaled = model(seq_tensor)\n",
        "                    pred_denorm = scaler_y.inverse_transform(pred_scaled.cpu().numpy().reshape(-1, 1)).flatten()\n",
        "                    predictions.extend(pred_denorm)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"   ì˜ˆì¸¡ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {e}\")\n",
        "                # í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
        "                mean_pred = scaler_y.inverse_transform([[0]])[0][0]\n",
        "                predictions.append(mean_pred)\n",
        "    else:\n",
        "        # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°\n",
        "        mean_pred = scaler_y.inverse_transform([[0]])[0][0]\n",
        "        predictions = [mean_pred] * len(X_test_scaled)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "# TimesNet ê±´ë¬¼ íƒ€ì…ë³„ ì˜ˆì¸¡\n",
        "timesnet_type_predictions = {}\n",
        "for building_type in timesnet_type_models.keys():\n",
        "    print(f\"   TimesNet ê±´ë¬¼ ìœ í˜• {building_type} ì˜ˆì¸¡ ì¤‘...\")\n",
        "    \n",
        "    type_test_data = test[test['building_type'] == building_type].sort_values(['building_number', 'date_time'])\n",
        "    \n",
        "    for building_num in type_test_data['building_number'].unique():\n",
        "        building_test_data = type_test_data[type_test_data['building_number'] == building_num]\n",
        "        \n",
        "        preds = predict_with_timesnet(\n",
        "            timesnet_type_models[building_type], \n",
        "            building_test_data, \n",
        "            available_features\n",
        "        )\n",
        "        \n",
        "        timesnet_type_predictions[building_num] = preds\n",
        "\n",
        "# TimesNet ê°œë³„ ê±´ë¬¼ë³„ ì˜ˆì¸¡\n",
        "timesnet_individual_predictions = {}\n",
        "for building_num in timesnet_individual_models.keys():\n",
        "    print(f\"   TimesNet ê±´ë¬¼ {building_num} ì˜ˆì¸¡ ì¤‘...\")\n",
        "    \n",
        "    building_test_data = test[test['building_number'] == building_num].sort_values('date_time')\n",
        "    \n",
        "    preds = predict_with_timesnet(\n",
        "        timesnet_individual_models[building_num], \n",
        "        building_test_data, \n",
        "        available_features\n",
        "    )\n",
        "    \n",
        "    timesnet_individual_predictions[building_num] = preds\n",
        "\n",
        "# CSV íŒŒì¼ ì €ì¥\n",
        "print(\"\\nğŸ’¾ TimesNet ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥...\")\n",
        "\n",
        "# TimesNet íƒ€ì…ë³„ ê²°ê³¼\n",
        "timesnet_type_submission = sample_submission.copy()\n",
        "for building_num, preds in timesnet_type_predictions.items():\n",
        "    mask = timesnet_type_submission['building_number'] == building_num\n",
        "    if len(preds) == mask.sum():\n",
        "        timesnet_type_submission.loc[mask, 'answer'] = preds\n",
        "\n",
        "timesnet_type_submission.to_csv('timesnet_type_model_submission.csv', index=False)\n",
        "print(\"âœ… TimesNet íƒ€ì…ë³„ ì˜ˆì¸¡ ì €ì¥: timesnet_type_model_submission.csv\")\n",
        "\n",
        "# TimesNet ê°œë³„ ê±´ë¬¼ ê²°ê³¼\n",
        "timesnet_individual_submission = sample_submission.copy()\n",
        "for building_num, preds in timesnet_individual_predictions.items():\n",
        "    mask = timesnet_individual_submission['building_number'] == building_num\n",
        "    if len(preds) == mask.sum():\n",
        "        timesnet_individual_submission.loc[mask, 'answer'] = preds\n",
        "\n",
        "timesnet_individual_submission.to_csv('timesnet_individual_model_submission.csv', index=False)\n",
        "print(\"âœ… TimesNet ê°œë³„ ê±´ë¬¼ ì˜ˆì¸¡ ì €ì¥: timesnet_individual_model_submission.csv\")\n",
        "\n",
        "# TimesNet ì•™ìƒë¸” (ê°œë³„ 7 : íƒ€ì… 3)\n",
        "print(\"\\nğŸ¯ TimesNet ì•™ìƒë¸” ì˜ˆì¸¡...\")\n",
        "timesnet_ensemble_submission = sample_submission.copy()\n",
        "\n",
        "for building_num in timesnet_ensemble_submission['building_number'].unique():\n",
        "    mask = timesnet_ensemble_submission['building_number'] == building_num\n",
        "    \n",
        "    individual_pred = timesnet_individual_predictions.get(building_num)\n",
        "    type_pred = timesnet_type_predictions.get(building_num)\n",
        "    \n",
        "    if individual_pred is not None and type_pred is not None:\n",
        "        if len(individual_pred) == len(type_pred) == mask.sum():\n",
        "            ensemble_pred = [0.7 * ind + 0.3 * typ for ind, typ in zip(individual_pred, type_pred)]\n",
        "            timesnet_ensemble_submission.loc[mask, 'answer'] = ensemble_pred\n",
        "    elif individual_pred is not None:\n",
        "        if len(individual_pred) == mask.sum():\n",
        "            timesnet_ensemble_submission.loc[mask, 'answer'] = individual_pred\n",
        "    elif type_pred is not None:\n",
        "        if len(type_pred) == mask.sum():\n",
        "            timesnet_ensemble_submission.loc[mask, 'answer'] = type_pred\n",
        "\n",
        "timesnet_ensemble_submission.to_csv('timesnet_ensemble_submission.csv', index=False)\n",
        "print(\"âœ… TimesNet ì•™ìƒë¸” ì˜ˆì¸¡ ì €ì¥: timesnet_ensemble_submission.csv\")\n",
        "\n",
        "# ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
        "print(\"\\nğŸ“Š TimesNet ëª¨ë¸ ê²°ê³¼ ìš”ì•½\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {NUM_FEATURES}ê°œ\")\n",
        "print(f\"TimesNet íƒ€ì…ë³„ ëª¨ë¸ ìˆ˜: {len(timesnet_type_models)}ê°œ\")\n",
        "print(f\"TimesNet ê°œë³„ ê±´ë¬¼ ëª¨ë¸ ìˆ˜: {len(timesnet_individual_models)}ê°œ\")\n",
        "\n",
        "if timesnet_type_scores:\n",
        "    avg_timesnet_type_smape = np.mean(list(timesnet_type_scores.values()))\n",
        "    print(f\"TimesNet íƒ€ì…ë³„ í‰ê·  SMAPE: {avg_timesnet_type_smape:.4f}\")\n",
        "\n",
        "if timesnet_individual_scores:\n",
        "    avg_timesnet_individual_smape = np.mean(list(timesnet_individual_scores.values()))\n",
        "    print(f\"TimesNet ê°œë³„ ê±´ë¬¼ í‰ê·  SMAPE: {avg_timesnet_individual_smape:.4f}\")\n",
        "\n",
        "print(\"\\nğŸ“ ì €ì¥ëœ TimesNet ê²°ê³¼ íŒŒì¼:\")\n",
        "print(\"- timesnet_type_model_submission.csv\")\n",
        "print(\"- timesnet_individual_model_submission.csv\")\n",
        "print(\"- timesnet_ensemble_submission.csv\")\n",
        "\n",
        "print(\"\\nğŸ‰ TimesNet ì „ë ¥ ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„ ì™„ë£Œ!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sd2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
